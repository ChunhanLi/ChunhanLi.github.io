---
layout:     post
title:      深度学习trick
subtitle:   
date:       2021-11-15
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 编程语言
---


### LookAhead

#### 使用
```python
base_optimizer = Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999))
opt = Lookahead(base_optimizer, k=5, alpha=0.5)
# 此时直接将opt作为正常的优化器使用即可，就像直接使用Adam一样的步骤使用opt
```
#### 参考
- https://zhuanlan.zhihu.com/p/75184359

### weight_decay

$$L = L_0 + \frac{\lambda}{2n}\sum_w w^2$$

L0代表原损失函数, 后一项是L2正则化项, n代表样本数

#### 参考
- https://www.pianshen.com/article/16621212014/

### 图像mixup

#### 使用

```python
# x是图像 z是dense特征等等
def mixup_data(x, z, y, params):
    # 直接处理一个batch 没毛病
    if params['mixup_alpha'] > 0:
        lam = np.random.beta(
            params['mixup_alpha'], params['mixup_alpha']
        )
    else:
        lam = 1

    batch_size = x.size()[0]
    if params['device'].type == 'cuda':
        index = torch.randperm(batch_size).cuda()
    else:
        index = torch.randperm(batch_size)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    mixed_z = lam * z + (1 - lam) * z[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, mixed_z, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

images, dense, target_a, target_b, lam = mixup_data(images, dense, target.view(-1, 1), params)
#参考 https://www.kaggle.com/micheomaano/efficientnet-b4-mixup-cv-0-98-lb-0-97

for data_ in tqdm(dataloaders['train']):
    x = data_['image']
    y = data_['target']
    if args.use_mixup and np.random.rand() <= args.mixup_prob:
        x, y_a, y_b, lam = mixup_data(x, y.view(-1, 1),0.5, use_cuda=True)
        x = x.to(device, dtype=torch.float,non_blocking = True)
        y_a = y_a.to(device, dtype=torch.float,non_blocking = True)
        y_b = y_b.to(device, dtype=torch.float,non_blocking = True)

        optimizer.zero_grad()
        preds = model(x)
        loss = mixup_criterion(loss_func, preds, y_a, y_b, lam)
    else:
        x = data_['image']
        y = data_['target']
        optimizer.zero_grad()
        x = x.to(device,non_blocking = True)
        y = y.to(device,non_blocking = True)
        preds = model(x)
        loss = loss_func(preds,y)   
    loss.backward()
    optimizer.step()
```
#### 参考

### Bag of Tricks for Image Classification with Convolutional Neural Networks

- https://github.com/FangYang970206/PaperNote/blob/master/CNN%20Tricks/Bag%20of%20Tricks%20for%20Image%20Classification%20with%20Convolutional%20Neural%20Networks.md
- https://www.cnblogs.com/fydeblog/p/12001296.html 【和上面一样】

1. 最后一层Bias可以初始化为 label均值对应的bias 加速收敛 但是不一定会使最后效果变好