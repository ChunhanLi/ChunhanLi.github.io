---
layout:     post
title:      逻辑回归
subtitle:   Logistic Regression
date:       2019-03-04
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 统计机器学习
---

## Logistic Regression

### 逻辑回归模型

#### Sigmoid函数

$$g(z) = \frac{1}{1+e^{-z}}$$

该函数将$z:(-\infty,\infty)$映射到了$(0,1)$上. z趋于正无穷时,$g(z)$趋于1. z趋于负无穷时，$g(z)$趋于0.

如果我们令$z = x^T\beta$,这样就得到了二元逻辑回归模型的一般形式:

$$h_{\beta}(x) = \frac{1}{1+e^{-x^T\beta}}$$

$$P(Y =1 \mid x) = h_{\beta}(x) = \frac{e^{x^T\beta}}{1 +e^{x^T\beta}}\\
P(Y =0 \mid x) = 1 - P(Y=1 \mid x) = \frac{1}{1+e^{x^T\beta}}$$

对于逻辑回归也可以这么去理解，

$$\text{logit}(P(Y=1\mid X)) = \log\frac{P(Y=1\mid X)}{1-P(Y=1\mid x)} = X^T\beta$$

### 逻辑回归损失函数

在二分类中，Y有两种编码形式。
- 0-1 coding
- -1,-1 coding

不同的编码形式对应的损失函数形式会有所不同,不过实质是一样的.

**0-1 coding**

可以把上面两个概率合并成一个概率：

$$P(Y = y \mid x) = \frac{1}{1+e^{x^T\beta}}e^{yx^T\beta}$$

逻辑回归采用的是对数损失函数:$-\log P(Y=y\mid x)$, 所有对于单个样本的损失函数：

$$l(x_i,y_i,\beta) = -y_ix_i^T\beta + \log(1 + e^{x_i^T\beta})$$

**-1,1 coding**

$$P(Y = 1\mid x) = \frac{1}{1+e^{-x^T\beta}}\\
P(Y=-1\mid x) = 1- P(Y =1 \mid x) = \frac{1}{1 + e^{x^T\beta}}$$

我们也可以结合两个式子:

$$P(Y =y \mid x) = \frac{1}{1+e^{-yx^T\beta}}$$

所以， 损失函数：

$$l(x_i,y_i,\beta) = \log(1 + e^{-yx^T\beta})$$

### 最大似然角度去理解损失函数

$Y\mid X$服从伯努利分布：

$$P(y = k|x) = p^k(1-p)^{(1-k)}$$

这里的p就是$P(Y = 1\mid x)$.

对数似然函数：

$$\sum_{i=1}^n\log(P(Y = y \mid x)) = \sum_{i=1}^n-\log(1+e^{x^T\beta})+yx^T\beta$$

将最大化对数似然转化成最小化问题，即得Min$\sum_{i=1}^nl_i(x_i,y_i,\beta)$

### Gradient and Hessian

首先，针对$l(y, x, \beta) = -yx^T\beta + \log(1+e^{x^T\beta})$. $x~~~ p\times1$维

Let $p$ denote $P(Y=1 \mid x)$

$$\frac{\partial l}{\partial \beta} = -yx + \frac{xe^{x^T\beta}}{1+e^{x^T\beta}} = -yx + xp = x(p-y)$$

$$\frac{\partial l}{\partial \beta \partial \beta^T} = x\frac{x^Te^{x^T\beta(1+e^{x^T\beta})}-e^{2x^T\beta}x^T}{(1+e^{x^T\beta})^2}=xx^Tp(1-p)$$

第二个式子是半正定的，说明了$l$是convex的.

$$\forall A,A^TXX^TA=(X^TA)^T(X^TA) \geq 0,\text{ 这里}X^TA\text{是一个数，不是矩阵，向量...}$$

$R_n(\beta)=\frac{1}{n}\sum_{i=1}^nl_i$

$$\text{Graident: }\frac{\partial R_n(\beta)}{\partial\beta} = \frac{1}{n}l_i = \frac{1}{n}(p_i-y_i)x_i==\frac{1}{n}X^T\gamma$$

$X~n\times p, r ~n\times 1,r_i=(p_i-y_i)$ 

同理, Hessian:

$$\text{Hessian: } \frac{\partial R_n}{\partial\beta\partial \beta^T}=\frac{1}{n}\sum_{i=1}^nx_ix^T_ip_i(1-p_i) == \frac{1}{n}X^T\omega X$$

$\omega$是对角矩阵，对角元素为$p_i(1-p_i)$

### 多元逻辑回归

To be continued.....

### Python code 实现

在这部分我会用三种办法去实现逻辑回归
- 牛顿法
- 批量梯度下降
- 随机梯度下降

[Jupter Notebook Python 实现](https://github.com/ChunhanLi/Machine-Learning-in-Action/blob/master/Study/LR/LR.ipynb)

### Sklearn包 

#### 参数分析

To be continued.....