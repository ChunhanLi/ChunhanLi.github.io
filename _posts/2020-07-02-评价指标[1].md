---
layout:     post
title:      评价指标[1]
subtitle:   
date:       2020-07-02
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 机器学习
---

### 评价指标

- TP 预测为1，真实为1
- FP 预测为1，真实为0
- TN 预测为0，真实为0
- FN 预测为0，真实为1

#### AUC
- AUC曲线纵坐标 $TPR = \frac{TP}{TP+FN}$,真正率 又名召回率(真实为1的有多少被找到)
- AUC曲线横坐标 $FPR = \frac{FP}{FP+TN}$ 假阳率(真实为0的有多少被误判)
- 为什么瞎猜的AUC为0.5?可以将人们将样本瞎猜为正的概率设为p，p又可以认为是划分正负样本阈值的一个参数,那么在理想情况下TPR=p,FRP=p,随着p增长 曲线面积为0.5
- AUC物理含义随机挑选出一个正样本和负样本,正样本的得分比负样本得分大的概率
- AUC计算公式,M为正样本数量,N为负样本数量,rank为得分从小到大的排名(分最小排第一)
$$AUC = \frac{\sum_{\text{正样本rank}}rank_i - \frac{M(M+1)}{2}}{M\times N}$$
- 上述公式 排名为rank最高的正样本 比(rank-1-(M-1))个的负样本高,排名为rank第二的正样本比(rank-1-(M-2))个的负样本高,以此类推,易得

#### KS值
- KS值定义是在roc-curve中的max(TPR-FPR)
- TPR代表找到坏人的比率,FPR代表把多少的好人误认为是坏人了,KS代表好人与坏人的区分程度

#### F1
- precision 精确率$\frac{TP}{TP+FP}$ 被预测出为1的里面有多少真为1
- recall之前说过
- P-R曲线 纵坐标 精确率 横坐标 召回率 不一定召回率是0的时候 精确率是1 反之也不一定
- F1 score $F1=\frac{2\times precision \times recall}{precision + recall}$

$$\frac{1}{F1} = 0.5 \times (\frac{1}{recall} + \frac{1}{precision})$$
- F1 macro:Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.
- F1 macro:即计算每一类类别的F1值,然后取平均
- F1 micro：比较繁琐一些,可以参考以下两个博客
- https://gongel.cn/?p=4506
- https://www.cnblogs.com/techengin/p/8962024.html
- 言简意赅, F1会计算出总TP,FP,FN值,针对每个类别加起来，再去计算F1。当分类没有分类错误(1234类 没有分到1234之外的类比如0类时) precision = recall = accraucy=F1
- 有分类错误时,FP代表预测为1类时 实际不为1类的数量 总和(这里1可以替换成1234) FN代表真实为1类 预测不为1