---
layout:     post
title:      决策树
subtitle:   Decision Tree
date:       2019-03-08
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 统计机器学习
---

[toc]

### 信息增益（熵）

熵(entropy)是表示随机变量不确定性的度量. 设X是一个取有限个值的离散随机变量,其概率分布为

$$P(X = x_i) = p_i,~~~~i = 1,2,3\dots,n$$

则随机变量X的熵的定义为

$$H(X)=-\sum_{i=1}^n p_i\log p_i$$

如果$p_i=0$，则定义$0\log0=0$.因为n比$\log n$高阶.通常log以e或者2为底.

将X的熵记做$H(p)$，$H(p)=-\sum_{i=1}^{n}p_i\log p_i$

熵越大,随机变量的不确定性就越大.从定义可验证(拉格朗日乘数法可以证明极大值)

$$0 \leq H(p) \leq \log n$$

随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)$H(Y\mid X)$, 定义为X给定条件下Y的条件概率分布的熵对X的数学期望

$$H(Y\mid X)=\sum_{i=1}^np_iH(Y\mid X=x_i)$$

信息增益(information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度.

**信息增益**

特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H（D）与特征A给定条件下D经验条件熵$H(D\mid A)$之差，即

$$g(D,A)=H(D) - H(D\mid A)$$


**信息增益比**

以信息增益作为划分训练数据集的特征,存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。

特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比,即

$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$

其中,$H_A(D)=-\sum_{i=1}^n\frac{\mid D_i\mid}{\mid D\mid}\log_2\frac{\mid D_i\mid}{\mid D\mid}$

### ID3算法

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征,递归地构建决策树.具体方法是:从根结点开始，对结点计算所有可能的特征的信息增益,选择信息增益最大的特征作为结点的特征,由该特征的不同取值建立子结点;再对子结点递归地调用以上方法,构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止. ID3相当于用极大似然法进行概率模型的选择.

**ID3算法**

- 输入：训练数据集D，特征集A，阈值$\epsilon$
- 输出: 决策树T

1. 若D中所有实例属于同一类$C_k$,则T为单结点树，并将类$C_k$作为该结点的类标记,返回T
2. 若A为空集,则T为单节点树,并将D中实例数最大的类$C_k$作为该结点的类标记,返回T
3. 否则，按照上面算法对各特征对D的信息增益,选择信息增益最大的特征$A_g$;
4. 如果$A_g$的信息增益小于阈值$\epsilon$,则置T为单节点树,并将D中实例最大的类$C_k$作为类标记,返回T；
5. 否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将D分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记,构建子节点，由结点及其子节点构成树T，返回T;
6. 对第i个子节点,以$D_i$为训练集,以A-{$A_g$}为特征集合，递归调用1-5步，得到子树$T_i$，返回$T_i$

#### ID3算法不足

1. 没有考虑连续特征
2. 取值多的特征一般比取值少的特征信息增益大，比较倾向于选择取值多的特征
3. 没有考虑缺失值
4. 没有考虑过拟合问题

### C4.5算法

C4.5算法改进了ID3的算法.

1. 采用二分法对连续特征离散化处理.比如连续特征有m个值,我们将其按顺序排好,两两取中点,进行划分，找出信息增益最大的划分方法.这种方法划分方法使用后的特征，在子树还可以继续使用,不同于离散特征. 在该算法下,计算量会大大增加.需要排序 遍历所以选择,一种微改进的办法是如果两个排序后连续特征之间只有一个类，那么不用在这两者之间考虑划分点.
2. 采用上面所定义的信息增益比作为选择特征的标准，其分母又称为分裂信息(split information)，用来惩罚取值较多的特征。但是如果在分裂信息中，某个类$D_i$大小和$D$接近时，分裂信息会趋于0，从而使信息增益比很大，即使信息增益不大.所以这里可以采用启发式算法，不直接使用增益率最大的作为划分属性,先从候选划分属性中找出信息增益高于平均水平的属性,再从中选择增益率最高的.
3. 对于这个问题需要解决两个问题
- 在属性值缺失的情况下如何选择划分特征？
- 给定划分特征,若样本在该属性上的值缺失，如何划分样本？

给定训练集D和属性a，令$\overset{\sim}{D}$表示D中在属性a上没有缺失值的样本字迹,对于问题1，我们可以用$\overset{\sim}{D}$表示D中在属性a的优劣.假定属性a有V个可取值$\{a^1,\dots,a^V\}$,令$\overset{\sim}{D}^v$表示$\overset{\sim}{D}$中在属性a上取值为$a^v$的样本子集,$\overset{\sim}{D}_k$表示$\overset{\sim}{D}$中属于第k类(k=1,2,$\dots$,Y)的样本子集。假定我们为每个样本x赋予一个权重$\omega_x$(在决策树学习的初始阶段,根结点的各样本权重初始化为1),并定义

$$p = \frac{\sum_{x\in \overset{\sim}{D}}\omega_x}{\sum_{x\in {D}}\omega_x}\\ \overset{\sim}{p}_k=\frac{\sum_{x\in\overset{\sim}{D}_k}\omega_x}{\sum_{x\in\overset{\sim}{D}}\omega_x}\\
\overset{\sim}{r}_v=\frac{\sum_{x \in \overset{\sim}{D}^v}\omega_x}{\sum_{x \in \overset{\sim}{D}}\omega_x}$$

对于问题1,我们可以定义(不太明白这个的动机 乘以p)

$$g(D,a) = p \times g(\overset{\sim}{D},a)$$

对于问题2，若样本x在划分属性a上的取值已知,则将x划入与其取值对应的子节点,且样本权值在子节点中保持为$\omega_x$.若样本x在划分属性a上未知,则将x同时划入所有子节点,且样本权值在与属性值$a^v$对应的子结点中调整为$\overset{\sim}{r}_v\omega_x$.直观上看,这就是让同一个样本以不同的概率划入到不同的子节点中去.

4. 我们会在Cart算法中仔细讨论剪枝问题.这里给出西瓜书上两种简单剪枝方法.

- 预剪枝:在决策树生成之前,对每个节点划分前进行估计,若当前节点的划分不能带来决策树泛华性能提升的话,则停止划分。
- 后剪枝：先从训练集生成的一颗完整的决策树,然后自底向上的对非叶节点进行考察,若将该节点对应的子树替换成叶节点是否能带来泛化能力的提升。

如何去评判能否带来提升，我们可以通过验证测试集的效果去对比。预剪枝使得决策树很多分支都没有展开，不仅降低了过拟合风险，还显著的减少了决策树的训练时间开销和测试时间开销。但是，有些特征虽然不能提升泛化性能，但是在其基础上的后续划分可能能带来明显的性能提高，所以预剪枝会带了欠拟合的风险。而对于后剪枝，它通常比预剪枝保留了更多的分支。一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝。但是后剪枝是在决策树生成之后，并且要自底向上地对树中所有非叶节点一一考察，因此其训练时间开销要大的多

#### C4.5算法不足

1. 只能分类，不能回归
2. 生成的可能是多叉树，在计算机中，不如二叉树有效率

### CART算法

全名叫分类与回归树(classificattion and regression tree, CART). 其为二叉树.

CART算法由以下两步组成:
1. 决策树的生成:基于训练数据集生成决策树,生成的决策树要尽量大；
2. 决策树剪枝:用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准

#### 回归树

回归树用平方误差最小化准则，进行特征选择，生成二叉树。

假设已将输入空间划分为M个单元$R_1,\dots,R_M$,并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为

$$f(x)=\sum_{m=1}^Mc_mI(x \in R_m)$$

当输入空间的划分确定时,可以用平方误差$\sum_{x_i \in R_m}(y_i - f(x_i))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。 易知，单元$R_m$上的$c_m$的最优值$\hat c_m$是$R_m$上所有输入实例$x_i$对应输出$y_i$的均值。

如何将空间进行划分？这里采用启发式的方法,选择第j个变量$x^{(j)}$和它的取值s作为切分变量和切分点,并定义两个区域:

$$R_1(j,s)=\{x \mid x^{(j)} \leq s\}\text{ 和 }R_2(j,s)=\{x \mid x^{(j)} >s\}$$

然后寻找最优切分变量j和最优切分点s。具体地，求解

$$\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$

对固定输入变量j可以找到最优切分点s。$c_1,c_2$对应其区域上均值.

遍历所有输入变量,找到最优的切分变量j,构成一个对(j,s).依次将输入空间划分为两个区域.接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成了一课回归树。这样的回归树通常称为最小二乘回归树。

**算法过程**
- 输入: 与训练数据集D
- 输出：回归树f(x).

在训练数据集所在的输入空间中,递归地将每个区域划分为两个子区域并决定每个子区域的输出值，构建二叉决策树:
  1. 选择最优切分变量j与切分点s，求解$$\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$

  遍历j，对固定的切分变量j扫描切分点s，选择使上式达到最小的值的对(j,s).
  2. 用选定的对(j,s)划分区域并决定相应的输出值
  3. 继续对两个子区域调用步骤(1),(2),直至满足停止条件.
  4. 将输入空间划分为M个区域$R_1,\dots,R_M$,生成决策树.

#### 分类树

分类树用基尼指数选择最优特征,同时决定该特征的最优二值切分点.

##### 基尼指数

分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为

$$\text{Gini}(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$

对于二分类问题,若样本点属于第一个类的概率是p，则概率分布的基尼指数为$\text{Gini}(p)=2p(1-p)$

如果样本集合D根据特征A是否取某一可能值a被分割成$D_1$和$D_2$两部分,则在特征A的条件下,集合D的基尼指数定义为

$$\text{Gini}(D,A) = \frac{\mid D_1\mid}{\mid D\mid}\text{Gini}(D_1)+\frac{\mid D_2\mid}{\mid D\mid}\text{Gini}(D_2)$$

基尼指数表示集合D的不确定性，基尼指数越大，不确定性越大,这与熵类似.在二分类情况下，基尼系数和熵的一半的曲线十分相近。可以说，基尼指数可以作为熵模型的一个近似替代.(因为对数运算相对耗时间?)

**Cart算法**

- 输入: 训练数据集D,停止计算的条件:
- 输出：CART决策树

根据训练数据集,从根结点开始，递归地对每个结点进行一下操作，构建二叉决策树:
  1. 计算现有特征对该数据集的基尼指数。此时，对每个特征A，对其可能的取的每一个值a，根据样本点对A是否=a将D分割为两部分，计算其A=a的基尼指数。
  2. 在所有可能的特征A以及其所有的切分点a中，选择基尼指数最小的特征及其对应的切分点。依据找到的特征以及切分点，将训练数据集依特征分配到两个子节点中区。
  3. 对两个子节点递归地调用1,2，直到满足停止条件。
  4. 生成CART决策树。

  算法的停止条件是结点的样本个数小于预定阈值，或者样本集的基尼指数小于预定阈值，或者没有更多特征。注意，这里与ID3,C4.5不同，比如A特征有三个类$a_1,a_2,a_3$，这次我们选用了$a_1$为最优特征，在之后的特征选择中，我们仍可将$a_2,a_3$作为判断特征去比较基尼指数。

### CART剪枝

#### 损失函数

T代表树T，其叶结点个数为$\mid T\mid$,t是树T的叶结点,该叶结点有$N_t$个样本点.

$$C_{\alpha}(T)=C(T)+\alpha\mid T\mid$$

C(T)表示模型对训练数据的预测误差,即模型与训练数据的拟合程度,$\mid T\mid$表示模型的复杂度。

如果预测误差考虑的是经验熵,那么


$$C(T)=\sum_{t=1}^{\mid T\mid}N_tH_t(T)$$
$H_t(T)$为叶结点t上的经验熵.

那么如果预测误差考虑的是基尼系数,那么（书上没明确给出公式，猜测是这样子）

$$C(T) = \sum_{t=1}^{\mid T\mid}N_t\text{Gini}_t(T)$$

$\text{Gini}_t(T)$为叶结点t上的基尼指数

回归树里的就用平方误差。

#### CART剪枝算法

CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。

也就是说CART剪枝算法由两步组成:首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点,形成一个子树序列{$T_0,T_1,\dots,T_n$};然后通过交叉验证法在独立的验证数据集上对子树序列进行测试,从中选择最优子树.

对固定的$\alpha$，一定存在使损失函数$C_{\alpha}(T)$最小的唯一子树。

接着，我们考虑一下剪枝过程.具体地,从整体树$T_0$开始剪枝。对$T_0$的任意内部结点t，以t为单结点树的损失函数是

$$C_{\alpha}(t)=C(t)+\alpha$$

以t为根结点的子树$T_t$的损失函数是

$$C_{\alpha}(T_t)=C(T_t)+\alpha\mid T_t\mid$$

当$\alpha=0$以及$\alpha$充分小的时候,有不等式

$$C_{\alpha}(T_t) < C_{\alpha}(t)$$

只要$\alpha = \frac{C(t)-C(T_t)}{\mid T\mid -1}$,$T_t$与$t$有相同的损失函数值，而t的结点少，因此t更可取，对$T_t$进行剪枝。

为此，对$T_0$中每一内部结点t，计算

$$g(t)=\frac{C(t)-C(T_t)}{\mid T_t \mid-1}$$

在整体树$T_0$中剪去$g(t)$最小的$T_t$，得到子树$T_1$,同时将最小的$g(t)$设为$\alpha_1$。$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树。如此增加$\alpha$，我们能得到一串子树$T_0,T_1,\dots,T_n$。 直至得到根结点.

**第二步**

在剪枝得到的子树序列中通过交叉验证选取最优子树$T_{\alpha}$（有点通过交叉验证去选择$\alpha$的意思）

具体地,利用独立的验证数据集,测试子树序列中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。

**CART剪枝算法**

输入：CART算法生成的决策树$T_0$
输出: 最优决策树$T_{\alpha}$
1. 设$k=0,T=T_0$
2. 设$\alpha = + \infty$
3. 自下而上地对各内部结点t计算$C(T_t),\mid T_t\mid$以及

$$g(t) = \frac{C(t)-C(T_t)}{\mid T_t\mid -1} \\
\alpha = \min(\alpha,g(t))$$

这里$T_t$表示以t为根结点的子树。

4. 对$g(t)=\alpha$的内部结点t进行剪枝,并对叶结点t以多数表决法决定其类，对于回归树确定均值，得到树T。

5. 设k=k+1，$\alpha_k = \alpha,T_k = T$
6. 如果$T_k$不是由根结点以及两个叶结点组成的树,则回到步骤2;否则$T_k = T_n$.
7. 采用交叉验证法在子树序列中选取最优子树

个人思考：刚开始看这个算法的时候有个疑问，为什么在步骤6中为什么还要回到步骤3再次去计算$\alpha$,为何不一开始将每个内部结点的$\alpha$的存下来，按其排序大小依次剪枝即可.我的想法是如果你得到了$T_1$，那么$T_2$就应该在把$T_1$当整体树的基础上进行，即在$T_1$中被剪枝的部分在$T_2$中也不会出现。

### 多变量决策树

这里不仔细介绍了。基本思想就是在选择特征时，不单单考虑一个特征会去考虑一个特征组合。

### 优缺点

#### 优点

1. 解释性强，直观
2. 不需要归一化，可处理缺失值

#### 缺点

1. 容易过拟合,在实际中需要设置节点最少样本数量和限制决策树深度来改进。
2. 寻找决策树是采用的是启发式算法，容易陷入局部最优。可以通过集成学习来改善。



### Python code

To be continued...

### Sklearn

To be continued...

### 参考

[他人博客](http://leijun00.github.io/2014/09/decision-tree/)

[刘建平博客](https://www.cnblogs.com/pinard/p/6050306.html#!comments)

统计学习方法

西瓜书
