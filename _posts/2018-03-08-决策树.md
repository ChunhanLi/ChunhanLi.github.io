---
layout:     post
title:      决策树
subtitle:   Decision Tree
date:       2019-03-08
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 统计机器学习
---

### 信息增益（熵）

熵(entropy)是表示随机变量不确定性的度量. 设X是一个取有限个值的离散随机变量,其概率分布为

$$P(X = x_i) = p_i,~~~~i = 1,2,3\dots,n$$

则随机变量X的熵的定义为

$$H(X)=-\sum_{i=1}^n p_i\log p_i$$

如果$p_i=0$，则定义$0\log0=0$.因为n比$\log n$高阶.通常log以e或者2为底.

将X的熵记做$H(p)$，$H(p)=-\sum_{i=1}^{n}p_i\log p_i$

熵越大,随机变量的不确定性就越大.从定义可验证(拉格朗日乘数法可以证明极大值)

$$0 \leq H(p) \leq \log n$$

随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)$H(Y\mid X)$, 定义为X给定条件下Y的条件概率分布的熵对X的数学期望

$$H(Y\mid X)=\sum_{i=1}^np_iH(Y\mid X=x_i)$$

信息增益(information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度. 

**信息增益** 

特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H（D）与特征A给定条件下D经验条件熵$H(D\mid A)$之差，即

$$g(D,A)=H(D) - H(D\mid A)$$


**信息增益比**

以信息增益作为划分训练数据集的特征,存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。

特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比,即

$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$

其中,$H_A(D)=-\sum_{i=1}^n\frac{\mid D_i\mid}{\mid D\mid}\log_2\frac{\mid D_i\mid}{\mid D\mid}$

### ID3算法

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征,递归地构建决策树.具体方法是:从根结点开始，对结点计算所有可能的特征的信息增益,选择信息增益最大的特征作为结点的特征,由该特征的不同取值建立子结点;再对子结点递归地调用以上方法,构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止. ID3相当于用极大似然法进行概率模型的选择.

**ID3算法**

- 输入：训练数据集D，特征集A，阈值$\epsilon$
- 输出: 决策树T

1. 若D中所有实例属于同一类$C_k$,则T为单结点树，并将类$C_k$作为该结点的类标记,返回T
2. 若A为空集,则T为单节点树,并将D中实例数最大的类$C_k$作为该结点的类标记,返回T
3. 否则，按照上面算法对各特征对D的信息增益,选择信息增益最大的特征$A_g$;
4. 如果$A_g$的信息增益小于阈值$\epsilon$,则置T为单节点树,并将D中实例最大的类$C_k$作为类标记,返回T；
5. 否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将D分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记,构建子节点，由结点及其子节点构成树T，返回T;
6. 对第i个子节点,以$D_i$为训练集,以A-{$A_g$}为特征集合，递归调用1-5步，得到子树$T_i$，返回$T_i$

#### ID3算法不足

1. 没有考虑连续特征
2. 取值多的特征一般比取值少的特征信息增益大，比较倾向于选择取值多的特征
3. 没有考虑缺失值
4. 没有考虑过拟合问题

### C4.5算法

C4.5算法改进了ID3的算法.

1. 采用二分法对连续特征离散化处理.比如连续特征有m个值,我们将其按顺序排好,两两取中点,进行划分，找出信息增益最大的划分方法.这种方法划分方法使用后的特征，在子树还可以继续使用,不同于离散特征. 在该算法下,计算量会大大增加.需要排序 遍历所以选择,一种微改进的办法是如果两个排序后连续特征之间只有一个类，那么不用在这两者之间考虑划分点.
2. 采用上面所定义的信息增益比作为选择特征的标准，其分母又称为分裂信息(split information)，用来惩罚取值较多的特征。但是如果在分裂信息中，某个类$D_i$大小和$D$接近时，分裂信息会趋于0，从而使信息增益比很大，即使信息增益不大.所以这里可以采用启发式算法，不直接使用增益率最大的作为划分属性,先从候选划分属性中找出信息增益高于平均水平的属性,再从中选择增益率最高的.
3. 
4. 对于过拟合问题,可以考虑剪枝,这个之后再详细讨论


### 参考

[他人博客](http://leijun00.github.io/2014/09/decision-tree/)
[刘建平博客](https://www.cnblogs.com/pinard/p/6050306.html#!comments)
统计学习方法
西瓜书