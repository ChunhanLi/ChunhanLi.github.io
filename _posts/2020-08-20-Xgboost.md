---
layout:     post
title:      Xgboost
subtitle:   
date:       2020-08-20
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 机器学习
---

#### 前言

前几天都在打比赛,比赛今天终于要结束咯,摸奖赛，Good luck.(2020.8.20)

摸了个第三名，还可以，决赛继续奋斗吧(2020.8.22)

#### Xgboost
- Xgboost 目标函数:$Obj = \sum_{i=1}^n l(\hat y_i,y_i) + \sum_{t=1}^k \Omega(f_t)$,k为树的棵数
- Xgboost也支持线性模型作为基模型(等价于带】L1/L2正则的逻辑回归或线性回归？)
- 泰勒公式$f(x+\Delta x) \approx f(x) + f^{\prime}(x)\Delta x + \frac{f^{\prime \prime}(x)}{2}\Delta x^2 $
- $Obj^t = \sum_{i=1}^n(l(y_i,\hat y_i^{t-1})+g_i f_t(x_i) + \frac{1}{2}h_if_t^2(x_i)) + \sum_{i=1}^t\Omega(f_i)$,t代表第几棵树
- 由于有些部分是常数,$Obj^t \approx \sum_{i=1}^n(g_i f_t(x_i) + \frac{1}{2}h_if_t^2(x_i)) + \Omega(f_i)$
- 我们可以将决策树定义为$f_t(x) = w_{q(x)}$,x为某一样本,这里的q(x)代表了该样本在哪个叶子结点上
- 决策树的复杂度由叶子数T组成,$\Omega(f_t) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2$
- $Obj^t = \sum_{j=1}^T(G_j w_j+\frac{1}{2}(H_j+\lambda)w_j^2)+\gamma T,G_j = \sum_{i \in I_j}g_i,H_j = \sum_{i \in I_j}h_i$
- $G_j$和$H_j$都是t-1步求导所得，可以视为常数，最后一棵树的叶子结点$w_j$不确定,求一阶导等于0，$w^*_j = -\frac{G_j}{H_j+\lambda}$,代入$Obj = -\frac{1}{2}\sum_{j=1}^T \frac{G_j^2}{H_j+\lambda}+\gamma   T$
- 假设在某一节点发生分裂,分裂前的目标函数可以写成$Obj_1 = -\frac{1}{2}(\frac{(G_L+G_R)^2}{H_L+H_R+\lambda })+\gamma$,分裂后的目标函数可以写成$Obj_2 = -\frac{1}{2}(\frac{G_L^2}{H_L+\lambda }+\frac{G_R^2}{H_R+\lambda})+2\gamma$,分裂后的收益为$$Gain = \frac{1}{2}(\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}) - \gamma$$
#### 几个关键点
- 学习率和列抽样
- 最优切分点划分算法 1. 贪心算法 按照上面的gain遍历所有特征(从小到大扫描)
- 近似算法
根据特征分布的分位数来分桶。有两种切分方式(gloal和local) 
global:学习每棵树前就提出切分点,并在每次分裂的时候都采用这种分裂
local:每次分裂时重新提出候选切分点
达到同样的预测效果,global比local需要更多个切分点
Xgboost不是使用简单的分位数,使用加权分位数(利用二阶导作为加权)
- 处理缺失值
为每个节点增加一个缺省的方向，在构建树节点的过程中只考虑非缺失值的数据遍历(一定程度上加快速度)，当数据缺失的时候归到缺省方向上，最优缺省方向
- 工程实现上
1. Xgboost在训练之前根据特征对数据进行了排序，然后保存在块结构中，每个块结构中都采用了稀疏矩阵存储格式，后面会重复使用块结构，可以大大减少计算量。各个块结构相互独立，各个特征的增益计算可以同时进行，方便分布式 多线程计算。
2. 缓存访问优化算法
3. “核外”块计算

#### 优缺点
优点
- **精度更高**：GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数；
- **灵活性更强**：GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，（使用线性分类器的 XGBoost 相当于带 L1 和 L2 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题））。此外，XGBoost 工具支持自定义损失函数，只需函数支持一阶和二阶求导；
- **正则化**：XGBoost 在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的 L2 范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合；
- **Shrinkage（缩减）**：相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间；
- **列抽样**：XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算；
- **缺失值处理**：XGBoost 采用的稀疏感知算法极大的加快了节点分裂的速度；
- **可以并行化操作**：块结构可以很好的支持并行计算。

缺点
- 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
- 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。

#### 常用可调参数
- n_estimators
- learning_rate
- max_depth
- colsample_bytree
is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.
- subsample 
无放回
- min_child_weight
Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. 
- gamma/min_split_loss
Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.

- reg_alpha
L1 regularization term on weights
- reg_lambda
L2 regularization term on weights.
- colsample_bylevel
is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. 
- colsample_bynode
is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated.
#### 其余参数
- importance_type

‘weight’: the number of times a feature is used to split the data across all trees.

‘gain’: the average gain across all splits the feature is used in.

‘cover’: the average coverage across all splits the feature is used in.

‘total_gain’: the total gain across all splits the feature is used in.

‘total_cover’: the total coverage across all splits the feature is used in.

- cover指的是这个特征影响了多少样本量的节点发生了改变

#### 参考
- https://zhuanlan.zhihu.com/p/87885678 （介绍直方图算法）
- https://stats.stackexchange.com/questions/422474/what-calculation-does-xgboost-use-for-feature-importances （特征重要性）
- https://datascience.stackexchange.com/questions/12318/how-do-i-interpret-the-output-of-xgboost-importance

- 
