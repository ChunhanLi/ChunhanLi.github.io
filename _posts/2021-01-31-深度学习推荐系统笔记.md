---
layout:     post
title:      深度学习推荐系统读书笔记
subtitle:   
date:       2021-01-31
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 机器学习
---


### 协同过滤(Collaborative Filtering)

得到用户与商品共现矩阵

1. 基于用户(UserCF)
- 得到topN相似用户，根据TopN用户对某商品的喜欢程度加权判断
- 缺点：1.用户数往往大于商品数 存储系统压力大 2.对于只有几次购买或者点击行为少的用户很难找到相似用户;大件商品的低频购买
2. 基于商品(ItemCF)
- 获得用户过去正反馈商品找出相似Topk物品 组成相似物品集合

#### 过滤协同的缺点
- 头部商品与其他商品的相似度均较高；头部效应明显


### 矩阵分解 

将用户对商品的打分矩阵做矩阵分解,其中该过程用梯度下降完成，用户商品都有个对应的隐向量，两者内积为对应打分，目标函数即是 内积和原始打分竟可能相似，还要加入偏差项，有点像embedding，但是还是无法加入用户、物品、上下文特征，例如年龄什么什么的；      

### FM
- 逻辑回归 二阶交叉的基础上 二阶交叉的权重由交叉特征的两个隐向量内积决定
### FFM
- 增加域的概念，和不同域交叉对应的隐向量不同
### Wide and deep

- wide专注于记忆能力memorization;deep专注于泛化能力Generalization
- wide part 广义线性模型 $y = W^T[X,\phi(X)]+b$.$\phi(x)$是交叉特征
- deep层是每个embedding的concat? 然后再接全连接
- Wide侧和Deep侧都准备好之后,对两部分输出进行简单加权求和即作为最终输出$P(Y=1\mid X) = \sigma(W^T_{wide}[X,\phi(X)]+W^T_{deep}a^{(l_f)}+b)$;$a^{(l_f)}$是deep侧最后一层激活函数输出结果
- Wide侧是高维稀疏,用FTRL算法优化;Deep侧用AdaGrad；(<font color='red'> 为什么捏？ </font>)

![image](https://pic1.zhimg.com/v2-1a968e1857e6fdd93fedbb6f6658e324_b.jpg)

### DeepFM
- https://zhuanlan.zhihu.com/p/94853056
- DeepFM模型结构与Wide&Deep很相似，二者最大的不同点在于，DeepFM两个部分共享底层输入特征，无需进行特征工程，而Wide&Deep模型两侧保持输入独立.

### 连续特征离散化输入LR的好处
1. 离散特征的增加和减少都很容易，易于模型的迭代
2. 离散化后对异常值有很强的鲁棒性
3. 稀疏向量内积乘法运算速度快
4. 单变量离散为N个后，每个变量拥有单独的权重，相当于模型引入了非线性
5. 方便特征交叉，引入非线性
6. 简化了逻辑回归模型，减少过拟合
7. 征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
### GBDT+LR
- GBDT和LR两步独立训练
- 每棵子树落在哪个叶子节点作为LR的离散值进入
### 大规模分段线性模型(LS-PLM)
- $f(x) = \sum_{i=1}^m\pi_i(x)\eta_i(x)=\sum_{i=1}^m\frac{e^{u_i x}}{\sum_{j=1}^m e^{u_j x}}\frac{1}{1+e^{-w_i  x}}$