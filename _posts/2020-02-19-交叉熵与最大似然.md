---
layout:     post
title:      交叉熵与最大似然
subtitle:   交叉熵与最大似然
date:       2020-08-15
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 神经网络
---
## MSE 
- MSE是假设数据服从高斯分布的时候等价于极大似然估计
- 正态分布
$$p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp({-\frac{(x - \mu)^2}{2\sigma^2}})$$
- 似然函数$L = \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp({-\frac{(y_i - \hat y_i)^2}{2\sigma^2}})$
- Max logL 等价于 极小化$\sum(y_i - \hat y_i)^2$
## 交叉熵
### 概念
- 信息量:$I(x) = -\log(p(x))$
- 熵(表示随机变量不确定性的度量/是所有可能发生事件产生信息量的期望):
$$H(x) = -\sum_{i=1}^np(x_i)\log(p(x_i))$$
- 相对熵(KL散度):用于衡量对于同一个随机变量X的两个分布p(x)和q(x)之间的差异.p对q的相对熵是:
$$D_{KL}(p\mid\mid q) = \sum_x p(x)\log\frac{p(x)}{q(x)} = E_{p(x)}\log\frac{p(x)}{q(x)}\\=\sum_x p(x)\log p(x) - \sum_x p(x)\log q(x)\\
\text{前者为常数,后者为交叉熵损失函数}$$ 



### 多分类交叉熵等价于极大似然

K类 $P(Y = j) = \beta_j$

$L = \prod_{i=1}^n\prod_{j=1}^K\beta_j^{I(Y_i=j)} = \prod_{i=1}^n\prod_{j=1}^KP(Y_i = j\mid X_i)^{I(Y_i=j)}$
$\log L = \prod_{i=1}^n\prod_{j=1}^K I(Y_i = j)\log P(Y_i=j\mid X_i)$

### 参考
- https://zhuanlan.zhihu.com/p/61944055
- https://zhuanlan.zhihu.com/p/84431551
- https://zhuanlan.zhihu.com/p/35709485