---
layout:     post
title:      交叉熵与最大似然
subtitle:   交叉熵与最大似然
date:       2020-02-19
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 神经网络
---

### 概念
- 信息量:$I(x) = -\log(p(x))$
- 熵(表示随机变量不确定性的度量/是所有可能发生事件产生信息量的期望):
$$H(x) = -\sum_{i=1}^np(x_i)\log(p(x_i))$$
- 相对熵(KL散度):用于衡量对于同一个随机变量X的两个分布p(x)和q(x)之间的差异.p对q的相对熵是:
$$D_{KL}(p\mid\mid q) = \sum_x p(x)\log\frac{p(x)}{q(x)} = E_{p(x)}\log\frac{p(x)}{q(x)}\\=\sum_x p(x)\log p(x) - \sum_x p(x)\log q(x)\\
\text{前者为常数,后者为交叉熵损失函数}$$ 


### 多分类交叉熵等价于极大似然

K类 $P(Y = j) = \beta_j$

$L = \prod_{i=1}^n\prod_{j=1}^K\beta_j^{I(Y_i=j)} = \prod_{i=1}^n\prod_{j=1}^KP(Y_i = j\mid X_i)^{I(Y_i=j)}$
$\log L = \prod_{i=1}^n\prod_{j=1}^K I(Y_i = j)\log P(Y_i=j\mid X_i)$