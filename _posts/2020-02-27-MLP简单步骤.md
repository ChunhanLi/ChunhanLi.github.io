---
layout:     post
title:      MLP简单步骤
subtitle:   MLP简单步骤
date:       2020-02-27
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 神经网络
---

### 预处理

For numerical features:
- log transform(挺好用的)
```python
np.log1p
```
- 减均值除以方差
```python
from sklearn.preprocessing import StandardScaler
X_train_num = X_train[num_features].copy()
sds = StandardScaler()
sds.fit(X_train_num)
X_train_num_ted = pd.DataFrame(sds.transform(X_train_num),columns=num_features)
X_test_num_ted = pd.DataFrame(sds.transform(X_test_num),columns=num_features)
```
- fill NAN by 0/mean
- add boolean indicators for all numerical features to indicate whether a certain feature is missing or not
```python
print(X_train.shape)
missing_boolen_list = []
for _ in num_features:
    if X_train[_].min()==-1:
        X_train[_+'missing_boolen'] = list(np.where(X_train[_]==-1,1,0))
        X_test[_+'missing_boolen'] = list(np.where(X_test[_]==-1,1,0))
        missing_boolen_list.append(_+'missing_boolen')
print(X_train.shape)
```

For Catrgorical features:
- cardinality is small. One hot
```python
one_hot = OneHotEncoder(sparse=False)
one_hot.fit(X_train_cat)
X_train_cat_ted = pd.DataFrame(one_hot.transform(X_train_cat),columns=one_hot.get_feature_names(['world','title']))###world/title是该类别的概称 自动生成worldxxxx特征名
X_test_cat_ted = pd.DataFrame(one_hot.transform(X_test_cat),columns=one_hot.get_feature_names(['world','title']))
```

- cardinality is large.
  - Let's say, if I have a feature of cardinality 10000 (10000 different classes), then I will just keep the most frequent 100 class (specific number is also a hyperparameter to tune) and label all other classes as 'others'. Then the cardinality is reduced from 10000 to 101. Then one hot

### 构建模型



### 临时
```python
history_dict = his.history
train_loss = history_dict['loss']
valid_loss = history_dict['val_loss']
plt.plot(his.epoch,train_loss,'bo',label = 'Training loss')
plt.plot(his.epoch,valid_loss,'r',label = 'Validation loss')
plt.xlabel('Epoches')
plt.ylabel('Loss')
plt.legend()
```

### BN/DROPOUT/dense层顺序问题
- https://blog.csdn.net/m0_37870649/article/details/82025238   CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->；
-https://www.zhihu.com/question/318354788 先BN再激活