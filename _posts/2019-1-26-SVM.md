---
layout:     post
title:      Support Vector Machines
subtitle:   SVM
date:       2019-1-26
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: false
tags:
    - 统计机器学习
---

[toc]
## Support Vector Machines

### 线性可分支持向量机与硬间隔最大化

训练数据集 $T = \{(x_1,y_1),\dots,(x_N,y_N)\}, y_i = \pm1$

一般地， 当训练集线性可分时， 存在无穷个分离超平面可将两类数据正确分开。 感知机利用误分类最小策略， 求得分离超平面， 解有无穷多个。 线性可分支持向量机利用间隔最大化求最优分离超平面，解释唯一的。

$$f(x) = \text{sign}(w^*x + b^*)$$

一个点距离分离超平面的远近可以表示其分类预测的确信程度。 如果点距离超平面比较远， 而且预测正确， 那么可说明该预测比较可信。

$\vert w\cdot x+b\vert$能够相对表示点x距离超平面的远近。  $y_i(w\cdot x_i+b)$可用来表示分类的正确性和确信度。

$$\hat \gamma_i = y_i(w\cdot x_i +b),\hat \gamma = \min_{i=1,\dots,N}\hat \gamma_i$$ 

$\gamma$为函数间隔的定义。但是在如此定义下， 如果w,b成比例的增大， 超平面没有改变，但是函数间隔却增大了。 所以， 我们可以对超平面做一些约束， 如规范化， 此时就引出了几何间隔的概念。

$$ \gamma_i = \frac{y_i(w\cdot x_i +b)}{||w||},\gamma = \min_{i=1,\dots,N}\gamma_i$$ 

注意， 几何间隔是实例点到超平面的带符号的距离。

支持向量机学习的基本思想就是使训练数据集能够正确的被分类而且几何间隔最大。

#### 硬间隔最大化

该问题可以表示为以下约束最优化问题：

$$\max_{w,b}\gamma \tag{1}\\ \text{s.t}~~~ y_i\frac{(w\cdot x_i + b)}{||w||} \geq\gamma$$

考虑几何间隔和函数间隔的关系， 问题改写成

$$\max_{w,b}\frac{\hat\gamma}{||w||} \tag{2}\\ \text{s.t}~~~ y_i{(w\cdot x_i + b)} \geq\hat\gamma $$

$\hat \gamma$的取值并不影响最优解。 可以这么看，如果把w,b按比例增加， 函数间隔也跟着随比例增加， 但这改变对上面约束没有影响。 所以我们可以取$\hat \gamma =1$， 得到下述等价优化问题， 这是一个凸二次规划问题。

$$\min_{w,b} \frac{1}{2}||w||^2 \\ \text{s.t.} ~y_i(w\cdot x_i +b) -1 \geq 0~~~i=1,\cdots,\tag 3$$

若数据集线性可分， 最大硬间隔超平面存在且唯一。证明略.

#### 支持向量和间隔边界

![svm](https://github.com/ChunhanLi/ChunhanLi.github.io/blob/master/img/svm1.jpg?raw=true)

在线性可分情况下， 训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量， 支持向量使约束条件(3.2， 即(3)的第二个式子)等号成立的点。在决定分离超平面时只有支持向量起作用， 而其他实例点并不起作用。

#### 对偶算法

应用对偶算法求解的优点：
- 对偶问题往往比较好求解(第一次取min时无不等式约束？原始问题第一次求max不等式约束？)
- 自然引进核函数（只考虑内积）

构造拉格朗日函数。
$$L(w,b,\alpha) = \frac{1}{2}\Vert w \Vert^2 - \sum_{i=1}^N\alpha_iy_i(wx_i+b) + \sum_{i=1}^N\alpha_i$$

对偶问题：

$$\max_{\alpha:\geq0}\min_{w,b}L(w,b,\alpha)$$

(1) min

$$\nabla_wL = w - \sum_{i=1}^N\alpha_iy_ix_i = 0, w =\sum_{i=1}^N\alpha_iy_ix_i \\\nabla_bL = \sum_{i=1}^N\alpha_iy_i = 0$$

(2)将（1）中两式代入 求对$\alpha$的极大值

$$\min_{w,b} L = -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i=1}^N\alpha_i$$

对上式求极大值等价于

$$\max_\alpha -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

转化为min

$$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

如何求解该优化问题，之后会介绍SMO算法。

**定理** 设 $\alpha^{\ast} = (\alpha_1^{\ast}, \alpha_2^{\ast}, \dots, \alpha_N^{\ast})^T$是对偶问题的解， 则存在下标j， 使得$\alpha_j^{\ast} >0$, 并可按下式求得原始最优问题的解$w^{\ast},b^{\ast}$:$$w^{\ast} = \sum_{i=1}^N\alpha_i^{\ast}y_ix_i\\b^{\ast} = y_j - \sum_{i=1}^N\alpha_i^{\ast}y_i(x_i \cdot x_j) \tag 4$$

证明: KKT条件成立：

$$\nabla_wL(w^*,b^*,a^*) = w^* - \sum_{i=1}^N\alpha_i^*y_ix_i = 0\\
\nabla_bL = -\sum_{i=1}^N\alpha_i^*y_i = 0\\y_i(w^*x_i + b^* )-1 \geq 0, ~~i =1,2,\dots\\\alpha_i^*(y_i(w^*x_i + b^* )-1) =0, i = 1,2,\dots\\\alpha_i^* \geq 0,~~ i=1,2,\dots \tag 5$$

$(5.1)$推出(4.1). 必有j， 使得$\alpha_j^* > 0$, 否则 $w^* = 0$. 由(5.4)得出

$$b^* = y_j - w^*x_j = y_j - \sum_{i=1}^N\alpha_i^*y_i(x_i \cdot x_j)$$

由（5.4）可知， 训练数据集中对应的$\alpha^{\ast}_i>0$的样本点 为支持向量.

**算法**（线性可分支持向量机学习算法）

(1)构造并求解约束最优化问题 

$$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

（2）用SMO算法求出上式子最小时对应的$\alpha^{\ast}$
（3）选择$\alpha^{\ast}$的一个正分量, 计算

$$w^* = \sum_{i=1}^N\alpha_i^*y_ix_i\\b^* = y_j - \sum_{i=1}^N\alpha_i^*y_i(x_i \cdot x_j) $$

（4）分类决策函数

$$f(x) = sign(w^*\cdot x + b^*)$$

### 线性支持向量机与软间隔最大化
#### 线性支持向量机
现实问题中， 分类数据集大多数都是线性不可分的。 这时我们需要引入松弛变量$\zeta_i \geq 0$.然后， 约束条件变为$y_i(w\cdot x_i +b) \geq 1 - \zeta_i$.对比与硬间隔最大化， 我们对距离的要求降低了. 当然对这个$\zeta$必须有所限制，我们将这种限制定义为代价, 即目标函数变成了$\frac{1}{2}||w||^2 + C\sum_{i=1}^N\zeta_i$. C(乘法参数)>0， 可以人为控制。 最小化目标函数有两层意义(1.使$||w||^2$尽量小， 即间隔尽量大， 同时使$\zeta_i$小， 从而使误分类的个数尽量小)

线性不可分的线性支持向量机的学习问题变成如下的凸二次规划问题(原始问题)：

$$\min_{w,b,\zeta}\frac{1}{2}\Vert w\Vert ^2 + C\sum_{i=1}^N\zeta_i\\ \text{s.t.} y_i(w \cdot x_i +b) \geq 1 - \zeta_i, ~~~i =1,2,\dots,N\\ \zeta_i \geq 0, i = 1,2,\dots,N$$

此问题解存在,w的解释唯一的,但b的解可能不唯一，而是存在于一个区间.

#### 学习的对偶算法
原始问题的拉格朗日问题

$$L(w,b,\zeta,\alpha,\mu) = \frac{1}{2}\Vert w \Vert^2 + C\sum_{i=1}^N \zeta_i - \sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1 + \zeta_i)- \sum_{i=1}^N \mu_i \zeta_i$$

其中,$\alpha_i, \mu_i \geq 0$

首先求其极小，

$$\nabla_w L = w - \sum_{i=1}^N\alpha_iy_ix_i = 0 \\
\nabla_b L= -\sum_{i=1}^N \alpha_iy_i = 0\\
\nabla_{\zeta_i}L = C - \alpha_i -\mu_i = 0$$

将上式子代入得到对偶问题：

$$\max_{\alpha} -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^N\alpha_i \\ \text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\ C-\alpha_i-\mu_i = 0\\ \alpha_i \geq 0 \\ \mu_i \geq 0 , i =1,2,\dots,N$$

对上式子后三项做变换， 可得$0 \leq \alpha_i \leq C $. 再将极大问题转化为极小问题,

$$\min \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum_{i=1}^N\alpha_i \\ \text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
0 \leq \alpha_i \leq C, i = 1,2,\dots,N$$

**定理** 设$\alpha^{\ast} = (\alpha_1^{\ast}, \dots, \alpha_n^{\ast})^T$是上述对偶问题的一个解, 若存在$\alpha^{\ast}$的一个分量$\alpha_j^{\ast}, 0<\alpha_j^{\ast}<C$,则原始问题的解$w^{\ast},b^{\ast}$可按下式求得:

$$w^{\ast} = \sum_{i=1}^N\alpha_i^{\ast}y_ix_i \\
b^{\ast} = y_j - \sum_{i=1}^Ny_i\alpha_i^{\ast}(x_i \cdot x_j)$$

证明: 原始问题是凸二次规划问题, 若满足KKT条件。 即得:

$$\nabla_w L = w^{\ast} - \sum_{i=1}^N\alpha_i^{\ast}y_ix_i = 0 \tag 6 \\ \nabla_bL = -\sum_{i=1}^N\alpha_i^{\ast}y_i = 0\\ \nabla_{\zeta}L =C- \alpha^{\ast} - \mu^{\ast} = 0 \\
\alpha_i^{\ast}(y_i(w^{\ast} \cdot x_i + b^{\ast}) - 1 + \zeta_i^{\ast}) = 0 \\
\mu_i^{\ast}\zeta_i^{\ast} = 0\\ y_i(w^{\ast} \cdot x_i + b^{\ast} ) -1 + \zeta_i^{\ast} \geq 0 \\ \zeta^{\ast} \geq 0\\ \alpha_i^{\ast} \geq 0\\ \mu_i^{\ast} \geq 0~~ i =1,\dots,N$$

由(6.1)(6.4)(6.5)(6.8)(6.9)可得上述定理成立



**算法（线性支持向量机学习算法）**

(1) 选择惩罚参数 C >0, 构造并求解凸二次规划问题

$$\max_{\alpha} -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^N\alpha_i \\ \text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\ C-\alpha_i-\mu_i = 0\\ \alpha_i \geq 0 \\ \mu_i \geq 0 , i =1,2,\dots,N$$

求得最优解$\alpha^{\ast} = (\alpha_1^{\ast},\dots,\alpha_N^{\ast})^T$.

(2)计算$w^{\ast} = \sum_{i=1}^N \alpha_i^{\ast}y_ix_i$

选择$\alpha_j^{\ast}, 0<\alpha_j^{\ast}<C$计算，

$$b^{\ast} = y_j - \sum_{i=1}^Ny_i\alpha_i^{\ast}(x_i \cdot x_j)$$

(3)求得分离超平面

$$w^{\ast}\cdot x + b^{\ast} =0$$

#### 支持向量

定义： 对于对应$\alpha_i^{\ast}>0$的样本点的实例$x_i$称为支持向量

支持向量几种情况:

- $\alpha_i^{\ast}<C,\text{那么} \zeta_i = 0 ~~~~x_i$落在间隔边界上
- 如果$\alpha_i^{\ast} = C, 0<\zeta_i<1$，则分类正确
- 如果$\alpha_i^{\ast} = C, \zeta_i= 1$,则在分离超平面上
- 如果$\alpha_i^{\ast} = C, zeta_i>1$，则落在分离超平面的另外一边

### 合页损失函数（Hinge Loss）

$$L(y(w\cdot x + b)) = [1-y(w\cdot x + b)]_+$$

**定理**

$$\min_{w,b,\zeta}\frac{1}{2}\Vert w\Vert ^2 + C\sum_{i=1}^N\zeta_i\\ \text{s.t.} y_i(w \cdot x_i +b) \geq 1 - \zeta_i, ~~~i =1,2,\dots,N \tag 7\\ \zeta_i \geq 0, i = 1,2,\dots,N $$

等价于最优化

$$\min_{w,b} \sum_{i=1}^N[1-y_i(w\cdot x_i + b)]_+ + \lambda \Vert w\Vert^2 \tag 8$$

证明： 可将 （8） 写成 （7）.令

$$[1-y_i(w\cdot x_i + b)]_+ = \zeta_i$$

该式子满足(7.2)(7.3),代入(8):

$$\min_{w,b}\sum_{i=1}^N\zeta_i + \lambda\Vert w \Vert^2$$

改变下系数，即可得到(7). 反之， （7）也能推出（8）.

#### Subgradient Method

To be continued.....

#### Surrogate loss
代替损失函数：一般指当目标函数非凸，不连续时，优化起来比较复杂， 这时需要使用其他性能较好的函数进行替换。 如果最优化代理损失函数的同时, 也能最优化了原来的损失函数， 我们就称其具有校对性（Calibration） 或者 一致性（Consistency）.

（对于0-1损失?）,一个重要定理， 如果代理损失函数是凸函数, 并且在0点可导, 导数小于0， 它一定具有一致性。 这也是我们通常选择凸函数作为我们loss function的一个原因。（其他原因：局部最小值等于全局最小值）

### 核技巧

现实中还有些问题是 非线性可分问题， 比如可以用一个超曲面将正负例正确分开。 此时所采取的办法就是进行一个非线性变换。

#### 核函数的定义
定义（核函数）： 设$X$是输入空间(欧式空间$R^n$的子集或离散集合)，设H 为特征空间(希尔伯特空间)， 如果存在一个从$X$到$H$的映射

$$\phi(x): X \rightarrow H$$

使得对所有x,z $\in X$, 函数K(x,z)满足条件

$$K(x,z) = \phi(x) \cdot \phi(z)$$

则称K（x,z)为核函数, $\phi(x)$为映射函数。

核技巧的想法是, 在学习与预测中只定义核函数K(x,z), 而不显式地定义映射函数$\phi$。（这样可以节省很多计算成本？）

通常所说的核函数就是正定核函数， 其充要条件如下：

设K：$X \times X \rightarrow R$是对称函数， 则K(x,z)为正定核函数的充要条件是 对于任意的$x_1,x_2,\dots,x_m \in X，$ K(x,z)关于$x_1,x_2,\dots,x_m \in X，$的Gram矩阵是半正定的。

#### SVM应用

分类决策函数变成:

$$f(x) = sign(\sum_{i=1}^N\alpha_i^{\ast}y_iK(x,x_i)+b^{\ast})$$
#### 常用核函数

- 多项式核函数(polynomial kernel function)

$$K(x,z) = (x \cdot z + 1)^p$$

对应的支持向量机是一个P次多项式分类器

- 高斯核函数(Gaussian kernel function)

$$K(x,z) = \exp(-\frac{\Vert x- z\Vert^2}{2 \sigma^2})$$

对应的支持向量机是高斯径向基函数(radial basis function)分类器

### 序列最小最优化算法


### 参考

- 统计学习方法 第七章
- 刘建平博客 https://www.cnblogs.com/pinard/p/6111471.html
- 凡哥博客 https://fan-gong.github.io/

