---
layout:     post
title:      Support Vector Machines
subtitle:   SVM
date:       2019-1-26
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 统计机器学习
---

[toc]
## Support Vector Machines

### 线性可分支持向量机与硬间隔最大化

训练数据集 $T = \{(x_1,y_1),\dots,(x_N,y_N)\}, y_i = \pm1$

一般地， 当训练集线性可分时， 存在无穷个分离超平面可将两类数据正确分开。 感知机利用误分类最小策略， 求得分离超平面， 解有无穷多个。 线性可分支持向量机利用间隔最大化求最优分离超平面，解释唯一的。

$$f(x) = \text{sign}(w^*x + b^*)$$

一个点距离分离超平面的远近可以表示其分类预测的确信程度。 如果点距离超平面比较远， 而且预测正确， 那么可说明该预测比较可信。

$\vert w\cdot x+b\vert$能够相对表示点x距离超平面的远近。  $y_i(w\cdot x_i+b)$可用来表示分类的正确性和确信度。

$$\hat \gamma_i = y_i(w\cdot x_i +b),\hat \gamma = \min_{i=1,\dots,N}\hat \gamma_i$$ 

$\gamma$为函数间隔的定义。但是在如此定义下， 如果w,b成比例的增大， 超平面没有改变，但是函数间隔却增大了。 所以， 我们可以对超平面做一些约束， 如规范化， 此时就引出了几何间隔的概念。

$$ \gamma_i = \frac{y_i(w\cdot x_i +b)}{||w||},\gamma = \min_{i=1,\dots,N}\gamma_i$$ 

注意， 几何间隔是实例点到超平面的带符号的距离。

支持向量机学习的基本思想就是使训练数据集能够正确的被分类而且几何间隔最大。

#### 硬间隔最大化

该问题可以表示为以下约束最优化问题：

$$\max_{w,b}\gamma \tag{1}\\ \text{s.t}~~~ y_i\frac{(w\cdot x_i + b)}{||w||} \geq\gamma$$

考虑几何间隔和函数间隔的关系， 问题改写成

$$\max_{w,b}\frac{\hat\gamma}{||w||} \tag{2}\\ \text{s.t}~~~ y_i{(w\cdot x_i + b)} \geq\hat\gamma $$

$\hat \gamma$的取值并不影响最优解。 可以这么看，如果把w,b按比例增加， 函数间隔也跟着随比例增加， 但这改变对上面约束没有影响。 所以我们可以取$\hat \gamma =1$， 得到下述等价优化问题， 这是一个凸二次规划问题。

$$\min_{w,b} \frac{1}{2}||w||^2 \\ \text{s.t.} ~y_i(w\cdot x_i +b) -1 \geq 0~~~i=1,\cdots,\tag 3$$

若数据集线性可分， 最大硬间隔超平面存在且唯一。证明略.

#### 支持向量和间隔边界

![svm](https://github.com/ChunhanLi/ChunhanLi.github.io/blob/master/img/svm1.jpg?raw=true)

在线性可分情况下， 训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量， 支持向量使约束条件(3.2， 即(3)的第二个式子)等号成立的点。在决定分离超平面时只有支持向量起作用， 而其他实例点并不起作用。

#### 对偶算法

应用对偶算法求解的优点：
- 对偶问题往往比较好求解(第一次取min时无不等式约束？原始问题第一次求max不等式约束？)
- 自然引进核函数（只考虑内积）

构造拉格朗日函数。
$$L(w,b,\alpha) = \frac{1}{2}\Vert w \Vert^2 - \sum_{i=1}^N\alpha_iy_i(wx_i+b) + \sum_{i=1}^N\alpha_i$$

对偶问题：

$$\max_{\alpha:\geq0}\min_{w,b}L(w,b,\alpha)$$

(1) min

$$\nabla_wL = w - \sum_{i=1}^N\alpha_iy_ix_i = 0, w =\sum_{i=1}^N\alpha_iy_ix_i \\\nabla_bL = \sum_{i=1}^N\alpha_iy_i = 0$$

(2)将（1）中两式代入 求对$\alpha$的极大值

$$\min_{w,b} L = -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i=1}^N\alpha_i$$

对上式求极大值等价于

$$\max_\alpha -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

转化为min

$$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

如何求解该优化问题，之后会介绍SMO算法。

**定理** 设 $\alpha^{\ast} = (\alpha_1^{\ast}, \alpha_2^{\ast}, \dots, \alpha_N^{\ast})^T$是对偶问题的解， 则存在下标j， 使得$\alpha_j^{\ast} >0$, 并可按下式求得原始最优问题的解$w^{\ast},b^{\ast}$:$$w^{\ast} = \sum_{i=1}^N\alpha_i^{\ast}y_ix_i\\b^{\ast} = y_j - \sum_{i=1}^N\alpha_i^{\ast}y_i(x_i \cdot x_j) \tag 4$$

证明: KKT条件成立：

$$\nabla_wL(w^*,b^*,a^*) = w^* - \sum_{i=1}^N\alpha_i^*y_ix_i = 0\\
\nabla_bL = -\sum_{i=1}^N\alpha_i^*y_i = 0\\y_i(w^*x_i + b^* )-1 \geq 0, ~~i =1,2,\dots\\\alpha_i^*(y_i(w^*x_i + b^* )-1) =0, i = 1,2,\dots\\\alpha_i^* \geq 0,~~ i=1,2,\dots \tag 5$$

$(5.1)$推出(4.1). 必有j， 使得$\alpha_j^* > 0$, 否则 $w^* = 0$. 由(5.4)得出

$$b^* = y_j - w^*x_j = y_j - \sum_{i=1}^N\alpha_i^*y_i(x_i \cdot x_j)$$

由（5.4）可知， 训练数据集中对应的$\alpha^{\ast}_i>0$的样本点 为支持向量.

**算法**（线性可分支持向量机学习算法）

(1)构造并求解约束最优化问题 

$$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

（2）用SMO算法求出上式子最小时对应的$\alpha^{\ast}$
（3）选择$\alpha^{\ast}$的一个正分量, 计算

$$w^* = \sum_{i=1}^N\alpha_i^*y_ix_i\\b^* = y_j - \sum_{i=1}^N\alpha_i^*y_i(x_i \cdot x_j) $$

（4）分类决策函数

$$f(x) = sign(w^*\cdot x + b^*)$$

### 线性支持向量机与软间隔最大化
#### 线性支持向量机
现实问题中， 分类数据集大多数都是线性不可分的。 这时我们需要引入松弛变量$\zeta_i \geq 0$.然后， 约束条件变为$y_i(w\cdot x_i +b) \geq 1 - \zeta_i$.对比与硬间隔最大化， 我们对距离的要求降低了. 当然对这个$\zeta$必须有所限制，我们将这种限制定义为代价, 即目标函数变成了$\frac{1}{2}||w||^2 + C\sum_{i=1}^N\zeta_i$. C(乘法参数)>0， 可以人为控制。 最小化目标函数有两层意义(1.使$||w||^2$尽量小， 即间隔尽量大， 同时使$\zeta_i$小， 从而使误分类的个数尽量小)

线性不可分的线性支持向量机的学习问题变成如下的凸二次规划问题(原始问题)：

$$\min_{w,b,\zeta}\frac{1}{2}\Vert w\Vert ^2 + C\sum_{i=1}^N\zeta_i\\ \text{s.t.} y_i(w \cdot x_i +b) \geq 1 - \zeta_i, ~~~i =1,2,\dots,N\\ \zeta_i \geq 0, i = 1,2,\dots,N$$

此问题解存在,w的解释唯一的,但b的解可能不唯一，而是存在于一个区间.

#### 学习的对偶算法
原始问题的拉格朗日问题

$$L(w,b,\zeta,\alpha,\mu) = \frac{1}{2}\Vert w \Vert^2 + C\sum_{i=1}^N \zeta_i - \sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1 + \zeta_i)- \sum_{i=1}^N \mu_i \zeta_i$$

其中,$\alpha_i, \mu_i \geq 0$

首先求其极小，

$$\nabla_w L = w - \sum_{i=1}^N\alpha_iy_ix_i = 0 \\
\nabla_b L= -\sum_{i=1}^N \alpha_iy_i = 0\\
\nabla_{\zeta_i}L = C - \alpha_i -\mu_i = 0$$

将上式子代入得到对偶问题：

$$\max_{\alpha} -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^N\alpha_i \\ \text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\ C-\alpha_i-\mu_i = 0\\ \alpha_i \geq 0 \\ \mu_i \geq 0 , i =1,2,\dots,N$$

对上式子后三项做变换， 可得$0 \leq \alpha_i \leq C $. 再将极大问题转化为极小问题,

$$\min \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) - \sum_{i=1}^N\alpha_i \\ \text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
0 \leq \alpha_i \leq C, i = 1,2,\dots,N$$

**定理** 设$\alpha^{\ast} = (\alpha_1^{\ast}, \dots, \alpha_n^{\ast})^T$是上述对偶问题的一个解, 若存在$\alpha^{\ast}$的一个分量$\alpha_j^{\ast}, 0<\alpha_j^{\ast}<C$,则原始问题的解$w^{\ast},b^{\ast}$可按下式求得:

$$w^{\ast} = \sum_{i=1}^N\alpha_i^{\ast}y_ix_i \\
b^{\ast} = y_j - \sum_{i=1}^Ny_i\alpha_i^{\ast}(x_i \cdot x_j)$$

证明: 原始问题是凸二次规划问题, 若满足KKT条件。 即得:

$$\nabla_w L = w^{\ast} - \sum_{i=1}^N\alpha_i^{\ast}y_ix_i = 0 \tag 6 \\ \nabla_bL = -\sum_{i=1}^N\alpha_i^{\ast}y_i = 0\\ \nabla_{\zeta}L =C- \alpha^{\ast} - \mu^{\ast} = 0 \\
\alpha_i^{\ast}(y_i(w^{\ast} \cdot x_i + b^{\ast}) - 1 + \zeta_i^{\ast}) = 0 \\
\mu_i^{\ast}\zeta_i^{\ast} = 0\\ y_i(w^{\ast} \cdot x_i + b^{\ast} ) -1 + \zeta_i^{\ast} \geq 0 \\ \zeta^{\ast} \geq 0\\ \alpha_i^{\ast} \geq 0\\ \mu_i^{\ast} \geq 0~~ i =1,\dots,N$$

由(6.1)(6.4)(6.5)(6.8)(6.9)可得上述定理成立



**算法（线性支持向量机学习算法）**

(1) 选择惩罚参数 C >0, 构造并求解凸二次规划问题

$$\max_{\alpha} -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i \cdot x_j) + \sum_{i=1}^N\alpha_i \\ \text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\ C-\alpha_i-\mu_i = 0\\ \alpha_i \geq 0 \\ \mu_i \geq 0 , i =1,2,\dots,N$$

求得最优解$\alpha^{\ast} = (\alpha_1^{\ast},\dots,\alpha_N^{\ast})^T$.

(2)计算$w^{\ast} = \sum_{i=1}^N \alpha_i^{\ast}y_ix_i$

选择$\alpha_j^{\ast}, 0<\alpha_j^{\ast}<C$计算，

$$b^{\ast} = y_j - \sum_{i=1}^Ny_i\alpha_i^{\ast}(x_i \cdot x_j)$$

(3)求得分离超平面

$$w^{\ast}\cdot x + b^{\ast} =0$$

#### 支持向量

定义： 对于对应$\alpha_i^{\ast}>0$的样本点的实例$x_i$称为支持向量

支持向量几种情况:

- $\alpha_i^{\ast}<C,\text{那么} \zeta_i = 0 ~~~~x_i$落在间隔边界上
- 如果$\alpha_i^{\ast} = C, 0<\zeta_i<1$，则分类正确
- 如果$\alpha_i^{\ast} = C, \zeta_i= 1$,则在分离超平面上
- 如果$\alpha_i^{\ast} = C, \zeta_i>1$，则落在分离超平面的另外一边

### 合页损失函数（Hinge Loss）

$$L(y(w\cdot x + b)) = [1-y(w\cdot x + b)]_+$$

**定理**

$$\min_{w,b,\zeta}\frac{1}{2}\Vert w\Vert ^2 + C\sum_{i=1}^N\zeta_i\\ \text{s.t.} y_i(w \cdot x_i +b) \geq 1 - \zeta_i, ~~~i =1,2,\dots,N \tag 7\\ \zeta_i \geq 0, i = 1,2,\dots,N $$

等价于最优化

$$\min_{w,b} \sum_{i=1}^N[1-y_i(w\cdot x_i + b)]_+ + \lambda \Vert w\Vert^2 \tag 8$$

证明： 可将 （8） 写成 （7）.令

$$[1-y_i(w\cdot x_i + b)]_+ = \zeta_i$$

该式子满足(7.2)(7.3),代入(8):

$$\min_{w,b}\sum_{i=1}^N\zeta_i + \lambda\Vert w \Vert^2$$

改变下系数，即可得到(7). 反之， （7）也能推出（8）.

#### Subgradient Method

To be continued.....

#### Surrogate loss
代替损失函数：一般指当目标函数非凸，不连续时，优化起来比较复杂， 这时需要使用其他性能较好的函数进行替换。 如果最优化代理损失函数的同时, 也能最优化了原来的损失函数， 我们就称其具有校对性（Calibration） 或者 一致性（Consistency）.

（对于0-1损失?）,一个重要定理， 如果代理损失函数是凸函数, 并且在0点可导, 导数小于0， 它一定具有一致性。 这也是我们通常选择凸函数作为我们loss function的一个原因。（其他原因：局部最小值等于全局最小值）

### 核技巧

现实中还有些问题是 非线性可分问题， 比如可以用一个超曲面将正负例正确分开。 此时所采取的办法就是进行一个非线性变换。

#### 核函数的定义
定义（核函数）： 设$X$是输入空间(欧式空间$R^n$的子集或离散集合)，设H 为特征空间(希尔伯特空间)， 如果存在一个从$X$到$H$的映射

$$\phi(x): X \rightarrow H$$

使得对所有x,z $\in X$, 函数K(x,z)满足条件

$$K(x,z) = \phi(x) \cdot \phi(z)$$

则称K（x,z)为核函数, $\phi(x)$为映射函数。

核技巧的想法是, 在学习与预测中只定义核函数K(x,z), 而不显式地定义映射函数$\phi$。（这样可以节省很多计算成本？）

通常所说的核函数就是正定核函数， 其充要条件如下：

设K：$X \times X \rightarrow R$是对称函数， 则K(x,z)为正定核函数的充要条件是 对于任意的$x_1,x_2,\dots,x_m \in X，$ K(x,z)关于$x_1,x_2,\dots,x_m \in X，$的Gram矩阵是半正定的。

#### SVM应用

分类决策函数变成:

$$f(x) = sign(\sum_{i=1}^N\alpha_i^{\ast}y_iK(x,x_i)+b^{\ast})$$

#### 常用核函数

- 多项式核函数(polynomial kernel function)

$$K(x,z) = (x \cdot z + 1)^p$$

对应的支持向量机是一个P次多项式分类器

- 高斯核函数(Gaussian kernel function)

$$K(x,z) = \exp(-\frac{\Vert x- z\Vert^2}{2 \sigma^2})$$

对应的支持向量机是高斯径向基函数(radial basis function)分类器

### 序列最小最优化算法

SMO算法要解如下凸二次规划的对偶问题:

$$\min_{\alpha} \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i\\ 
\text{ s.t. } \sum_{i=1}^N\alpha_iy_i=0 \\
0 \leq \alpha_i \leq C, ~~ i=1,2,\dots,N$$

SMO是一种启发式算法，其基本思路是如果所有变量的解都满足KKT条件，那么最优化问题的解救得到了。 否则， 选择两个变量， 固定其他变量， 构建二次规划， 求解二次规划的最优解， 这个解就会更接近与原始二次规划的解。 而且SMO算法中有约束条件，所以其他相当于只有一个变量。对其求导即可。

在这个条件下，上述问题可以写成:

$$\min_{\alpha_i,\alpha_j} W(\alpha_i,\alpha_j) = \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1\alpha_2\\~~~~~~~~~~~~~~~~~~~~~~~-(\alpha_1+\alpha_2)+y_1\alpha_1\sum_{i=3}^Ny_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_iK_{i2}\\ \text{s.t. } \alpha_1y_1 + \alpha_2y_2 = -\sum_{i=3}^Ny_i\alpha_i=\zeta\\
0 \leq \alpha_i \leq C,~i=1,2$$

其中, $K_{ij}=K(x_i,x_j)$

首先先考虑其约束条件， 其自由度为1， 我们就考虑关于$\alpha_2$的优化范围。

如果$y_1 \neq y_2$,则

$$L =\max(0, \alpha_2^{old}-\alpha_1^{old}),~ H = \min(C, C+ \alpha_2^{old}-\alpha_1^{old})$$

如果$y_1 = y_2$,则

$$L = \max(0, \alpha_2^{old}+\alpha_1^{old}-C),~~H=\min(C, \alpha_2^{old}+\alpha_1^{old})$$

具体过程见下图。

![image](https://github.com/ChunhanLi/ChunhanLi.github.io/blob/master/img/svm_smo.png?raw=true)

下面我们考虑不在前面考虑的约束条件下，得$\alpha_2$的最优解$\alpha_2^{new,unc}$;然后再求约束下的解$\alpha_2^{new}$.

为了叙述简单, 记

$$g(x)=\sum_{i=1}^N\alpha_iy_iK(x_i,x)+b$$

令

$$E_i = g(x_i) - y_i$$

**定理** 上述最优化问题沿着约束方向($\sum_{i=1}^N\alpha_iy_i=0$)的未经剪辑时（不限制大于等于0小于等于C）的解是

$$\alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1-E_2)}{\eta}$$

其中,

$$\eta=K_{11}+K_{22}-2K_{12}=\mid\mid\Phi(x_1)-\Phi(x_2)\mid\mid^2$$

证明见统计学习方法(p.127-p.129)

经剪辑后$\alpha_2$的解是

$$\alpha_2^{new}=\left\{\begin{array}{cc} 
		H, & \alpha_2^{new,unc}>H\\ 
		\alpha_2^{new,unc}, & L \leq \alpha_2^{new,unc}\leq H\\
        L, & \alpha_2^{new,unc}<L 
	\end{array}\right.$$

由$\alpha_2^{new}y_2 +\alpha_1^{new}y_1=\alpha_2^{old}y_2+\alpha_1^{old}y_1$得:

$$\alpha_1^{new}=\alpha_1^{old}+y_1y_2(\alpha^{old}_2-\alpha_2^{new})$$

#### 变量的选择方法

**1.第一个变量的选择**

SMO称选择第一个变量的过程为外层循环，外层循环在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第一个变量。具体地，检验训练样本点($x_i,y_i$)是否满足KKT条件，即

$$\alpha_i = 0 \leftarrow\rightarrow y_ig(x_i)\geq 1\\
0<\alpha_i<C \leftarrow\rightarrow y_ig(x_i)=1\\
\alpha_i=C \leftarrow\rightarrow y_ig(x_i)\leq 1$$

其中, $g(x_i)=\sum_{j=1}^N\alpha_jy_jK(x_i,x_j)+b$

该检验是在$\epsilon$范围内进行的。 在检验过程中, 外层循环首先遍历所以满足$0<\alpha_i<C$的样本点，即在间隔边界上的支持向量点,检验它们是否满足KKT条件。如果这些样本点都满足KKT条件,那么遍历整个训练集,检验它们是否满足KKT条件。

**2.第二个变量的选择**

SMO称选择第2个变量的过程为内层循环。假设在外层循环中已经找到第一个变量$\alpha_1$，现在要在内层循环中找第2个变量$\alpha_2$.第2个变量选择的标准是希望能使$\alpha_2$有足够大的变化(我的理解是$\alpha_2$变化足够大，不满足KKT的$\alpha_1$也能足够变化，而且变化大加快迭代速率)

$\alpha_2^{new}$是依赖于$\mid E_1 - E_2\mid$的一种简单的做法是选择$\alpha_2$使其对应的$\mid E_1 - E_2\mid$最大。$\alpha_1$定了,$E_1$也确定了。如果$E_1$是正的,那么选择最小的$E_i$作为$E_2$，如果负的，选择最大的。(这里有个疑问,$E_1$为正时，也有可能最大的$E_2$使其差绝对值最大。实际操作可以都考虑，应该也不会增加太多计算时间)

在特殊情况下,如果内层的循环通过以上方法选择的$\alpha_2$不能使目标函数有足够的下降,那么采用以下启发式规则继续选择$\alpha_2$。 遍历在间隔边界上的支持向量点,依次将其作为$\alpha_2$试用,直到目标函数有足够的下降。若找不到合适的$\alpha_2$，那么遍历训练数据集;若仍找不到，则跳出循环,寻找下一个$\alpha1$.

**3.计算阈值b和差值$E_i$**

在每次完成两个变量的优化后,都要重新计算阈值b.当$0<\alpha_1^{new}<C$时, 由$y_ig(x_i)=1$可知：

$$\sum_{i=1}^N\alpha_iy_iK_{i1}+b=y_1$$

于是，

$$b_1^{new}=y_1-\sum_{i=3}^N\alpha_iy_iK_{i1}-\alpha_1^{new}y_1K_{11}-\alpha_2^{new}y_2K_{21}$$

由$E_1$的定义可知,

$$E_1=\sum_{i=1}^N\alpha_iy_iK_{i1}+\alpha_1^{old}y_1K_{11}+\alpha_2^{old}y_2K_{21}+b^{old}-y_1$$

合并前两项可得:

$$b_1^{new}=-E_1-y_1K_{11}(\alpha_1^{new}-\alpha_1^{old})-y_2K_{21}(\alpha_2^{new}-\alpha_2^{old})+b^{old}$$

同样如果$0<\alpha_2^{new}<C$,那么,

$$b_2^{new}=-E_2-y_1K_{12}(\alpha_1^{new}-\alpha_1^{old})-y_2K_{22}(\alpha_2^{new}-\alpha_2^{old})+b^{old}$$

如果$\alpha_1^{new},\alpha_2^{new}$同时满足在（0,C）之间的条件，那么$b_1^{new}=b_2^{new}$.如果他两都在界上,$b_1^{new}$和$b_2^{new}$及其中间的值都符合KKT，可以取他们中点作为b。（这段话不太理解...）

每次还得记得更新$E_i$.其更新需要用到新的$b^{new}$

#### SMO总结

输入:训练数据集T.

输出:近似解$\hat \alpha$

(1) 取初值$\alpha^{(0)}=0，k=0$

(2) 选取优化变量$\alpha_1^{(k)},\alpha_2^{(k)}$,按照上述方法求解$\alpha_1^{(k+1)},\alpha_2^{(k+1)}$.

(3) 计算$b^{k+1}$和$E_i$

(4)若在精度$\epsilon$范围内满足停止条件

$$\sum_{i=1}^N\alpha_iy_i=0 \\
0 \leq \alpha_i \leq C,~ i=1,2,\dots,N\\
\alpha_i = 0 \leftarrow\rightarrow y_ig(x_i)\geq 1\\
0<\alpha_i<C \leftarrow\rightarrow y_ig(x_i)=1\\
\alpha_i=C \leftarrow\rightarrow y_ig(x_i)\leq 1$$

(5)满足停机条件则结束，否则返回(2)

### Python Code

[Jupyter Notebook Python实现](https://github.com/ChunhanLi/Machine-Learning-in-Action/blob/master/Study/CH%206/SVM.ipynb)

### sklearn.svm.SVC

#### 参数分析

To be continued.....

### 参考

- 统计学习方法 第七章
- [刘建平博客](https://www.cnblogs.com/pinard/p/6111471.html)
- [凡哥博客](https://fan-gong.github.io/)

