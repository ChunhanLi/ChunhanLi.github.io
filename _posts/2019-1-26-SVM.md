---
layout:     post
title:      Support Vector Machines
subtitle:   SVM
date:       2019-1-26
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: false
tags:
    - 统计机器学习
---

[toc]
## Support Vector Machines

### 线性可分支持向量机与硬间隔最大化

训练数据集 $T = \{(x_1,y_1),\dots,(x_N,y_N)\}, y_i = \pm1$

一般地， 当训练集线性可分时， 存在无穷个分离超平面可将两类数据正确分开。 感知机利用误分类最小策略， 求得分离超平面， 解有无穷多个。 线性可分支持向量机利用间隔最大化求最优分离超平面，解释唯一的。

$$f(x) = \text{sign}(w^*x + b^*)$$

一个点距离分离超平面的远近可以表示其分类预测的确信程度。 如果点距离超平面比较远， 而且预测正确， 那么可说明该预测比较可信。

$|w\cdot x+b|$能够相对表示点x距离超平面的远近。  $y_i(w\cdot x_i+b)$可用来表示分类的正确性和确信度。

$$\hat \gamma_i = y_i(w\cdot x_i +b),\hat \gamma = \min_{i=1,\dots,N}\hat \gamma_i$$ 

$\gamma$为函数间隔的定义。但是在如此定义下， 如果w,b成比例的增大， 超平面没有改变，但是函数间隔却增大了。 所以， 我们可以对超平面做一些约束， 如规范化， 此时就引出了几何间隔的概念。

$$ \gamma_i = \frac{y_i(w\cdot x_i +b)}{||w||},\gamma = \min_{i=1,\dots,N}\gamma_i$$ 

注意， 几何间隔是实例点到超平面的带符号的距离。

支持向量机学习的基本思想就是使训练数据集能够正确的被分类而且几何间隔最大。

#### 硬间隔最大化

该问题可以表示为以下约束最优化问题：

$$\max_{w,b}\gamma \tag{1}\\ \text{s.t}~~~ y_i\frac{(w\cdot x_i + b)}{||w||} \geq\gamma$$

考虑几何间隔和函数间隔的关系， 问题改写成

$$\max_{w,b}\frac{\hat\gamma}{||w||} \tag{2}\\ \text{s.t}~~~ y_i{(w\cdot x_i + b)} \geq\hat\gamma $$

$\hat \gamma$的取值并不影响最优解。 可以这么看，如果把w,b按比例增加， 函数间隔也跟着随比例增加， 但这改变对上面约束没有影响。 所以我们可以取$\hat \gamma =1$， 得到下述等价优化问题， 这是一个凸二次规划问题。


$$
\begin{aligned}
&\min_{w,b} \frac{1}{2}||w||^2 \\
\text{s.t.} ~y_i(w\cdot x_i &+b) -1 \geq 0~~~i=1,\cdots,N& \tag 3\\ 
\end{aligned}
$$

若数据集线性可分， 最大硬间隔超平面存在且唯一。证明略.

#### 支持向量和间隔边界

![svm](https://github.com/ChunhanLi/ChunhanLi.github.io/blob/master/img/svm1.jpg?raw=true)

在线性可分情况下， 训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量， 支持向量使约束条件(3.2， 即(3)的第二个式子)等号成立的点。在决定分离超平面时只有支持向量起作用， 而其他实例点并不起作用。

#### 对偶算法

应用对偶算法求解的优点：
- 对偶问题往往比较好求解(第一次取min时无不等式约束？原始问题第一次求max不等式约束？)
- 自然引进核函数（只考虑内积）

构造拉格朗日函数。$$L(w,b,\alpha) = \frac{1}{2}||w||^2 - \sum_{i=1}^N\alpha_iy_i(wx_i+b) + \sum_{i=1}^N\alpha_i$$

对偶问题：$$\max_{\alpha:\geq0}\min_{w,b}L(w,b,\alpha)$$

(1) min
$$\nabla_wL = w - \sum_{i=1}^N\alpha_iy_ix_i = 0, w =\sum_{i=1}^N\alpha_iy_ix_i \\\nabla_bL = \sum_{i=1}^N\alpha_iy_i = 0$$

(2)将（1）中两式代入 求对$\alpha$的极大值
$$\min_{w,b} L = -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i=1}^N\alpha_i$$

对上式求极大值等价于$$\max_\alpha -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

转化为min

$$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

如何求解该优化问题，之后会介绍SMO算法。

**定理** 设 $\alpha^* = (\alpha_1^*, \alpha_2^*, \dots, \alpha_N^*)^T$是对偶问题的解， 则存在下标j， 使得$\alpha_j^* >0$, 并可按下式求得原始最优问题的解$w^*,b^*$:$$w^* = \sum_{i=1}^N\alpha_i^*y_ix_i\\b^* = y_j - \sum_{i=1}^N\alpha_i^*y_i(x_i \cdot x_j) \tag 4$$

证明: KKT条件成立：
$$\nabla_wL(w^*,b^*,a^*) = w^* - \sum_{i=1}^N\alpha_i^*y_ix_i = 0\\
\nabla_bL = -\sum_{i=1}^N\alpha_i^*y_i = 0\\y_i(w^*x_i + b^* )-1 \geq 0, ~~i =1,2,\dots\\\alpha_i^*(y_i(w^*x_i + b^* )-1) =0, i = 1,2,\dots\\\alpha_i^* \geq 0,~~ i=1,2,\dots \tag 5$$

$(5.1)$推出(4.1). 必有j， 使得$\alpha_j^* > 0$, 否则 $w^* = 0$. 由(5.4)得出$$b^* = y_j - w^*x_j = y_j - \sum_{i=1}^N\alpha_i^*y_i(x_i \cdot x_j)$$

由（5.4）可知， 训练数据集中对应的$\alpha^*_i>0$的样本点 为支持向量.

**算法**（线性可分支持向量机学习算法）

(1)构造并求解约束最优化问题 $$\min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i\\\text{s.t.} \sum_{i=1}^N\alpha_iy_i = 0\\
\alpha_i \geq 0 , i = 1,2,\dots,N$$

（2）用SMO算法求出上式子最小时对应的$\alpha^*$
（3）选择$\alpha^*$的一个正分量, 计算$$w^* = \sum_{i=1}^N\alpha_i^*y_ix_i\\b^* = y_j - \sum_{i=1}^N\alpha_i^*y_i(x_i \cdot x_j) $$
（4）分类决策函数$$f(x) = sign(w^*\cdot x + b^*)$$

### 线性支持向量机与软间隔最大化
#### 线性支持向量机
现实问题中， 分类数据集大多数都是线性不可分的。 这时我们需要引入松弛变量$\zeta_i \geq 0$.然后， 约束条件变为$y_i(w\cdot x_i +b) \geq 1 - \zeta_i$.对比与硬间隔最大化， 我们对距离的要求降低了. 当然对这个$\zeta$必须有所限制，我们将这种限制定义为代价, 即目标函数变成了$\frac{1}{2}||w||^2 + C\sum_{i=1}^N\zeta_i$. C(乘法参数)>0， 可以人为控制。 最小化目标函数有两层意义(1.使$||w||^2$尽量小， 即间隔尽量大， 同时使$\zeta_i$小， 从而使误分类的个数尽量小)

线性不可分的线性支持向量机的学习问题变成如下的凸二次规划问题(原始问题)：$$\min_{w,b,\zeta}\frac{1}{2}||w||^2 + C\sum_{i=1}^N\zeta_i\\ \text{s.t.} y_i(w \cdot x_i +b) \geq 1 - \zeta_i, ~~~i =1,2,\dots,N\\ \zeta_i \geq 0, i = 1,2,\dots,N$$

此问题解存在,w的解释唯一的,但b的解可能不唯一，而是存在于一个区间.

#### 学习的对偶算法
原始问题的拉格朗日问题$$L(w,b,\zeta,\alpha,\mu) = $$
### 参考

- 统计学习方法 第七章
- 刘建平博客 https://www.cnblogs.com/pinard/p/6111471.html
- 凡哥博客 https://fan-gong.github.io/

