---
layout:     post
title:      Python_ML
subtitle:   常用代码存储
date:       2019-10-19
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 编程语言
---


### 前言

这个章节会记录一些机器学习比赛中常用的一些套路模板


#### count encoding
```python
for i in count_columns:
    train[i+'_count_full'] = train[i].map(pd.concat([train[i], test[i]], ignore_index=True).value_counts(dropna=False))
    test[i+'_count_full'] = test[i].map(pd.concat([train[i], test[i]], ignore_index=True).value_counts(dropna=False))
```

#### rolling function
```python
## https://blog.csdn.net/maymay_/article/details/80241627
```

#### define eval_function in sklearn
```python
# https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/sklearn_example.py
# self-defined eval metric
# f(y_true: array, y_pred: array) -> name: string, eval_result: float, is_higher_better: bool
# Root Mean Squared Logarithmic Error (RMSLE)
def rmsle(y_true, y_pred):
    return 'RMSLE', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False
def rae(y_true, y_pred):
    return 'RAE', np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(np.mean(y_true) - y_true)), False

#### 分类问题 y_prob.reshape((len(y_true),-1),order='F') 
### 需要这个操作 返回的y_prob是概率 不是预测值
def CRPS_eval(y_true, y_prob):
    y_prob =  y_prob.reshape((len(y_true),-1),order='F')
    global a
    a=y_prob.copy()
    
    y_pred_array1 = raw_y_to_array2(y_prob,num_counts,bins)
    if len(y_true) < y_train.shape[0]/3:
        index1 = test_index.copy()
    else:
        index1 = train_index.copy()
    return 'CRPS', cal_CPRS(y_true_array1[index1,:],y_pred_array1), False
```

### use two metric and early stop at one
```python
def rmse(y_true, y_pred):
    return 'RMSE', np.sqrt(np.mean(np.power((y_pred) - (y_true), 2))), False
lgb_re = lgb.LGBMRegressor(n_estimators=10000, random_state=51,subsample=0.8,
                         colsample_bytree=0.8,learning_rate=0.1,importance_type = 'gain',
                 max_depth = -1, num_leaves = 64,metric='None',bagging_freq=1,n_jobs=-1,first_metric_only = True)
lgb_re.fit(X_train,y_train,eval_set = [(X_valid,y_valid)],verbose=50,
           eval_metric =lambda y_true, y_pred: [evaluator.feval(y_true, y_pred), rmse(y_true, y_pred)],
           early_stopping_rounds=100 )#,categorical_feature=cat_col)
```

#### 将列值转换成列名
```python
example = pd.DataFrame({'Team':['team1','team2']*3,'score':[1,2,3,4,5,6],'times':[6,5,3,2,1,1],'id':[1,1,1,1,1,1]
                        ,'player':['player1','player1','player2','player2','player3','player3']})
example
temp1 = pd.pivot_table(example,columns=['Team','player'],aggfunc=lambda x:x,index=['id'])
temp1
temp1.columns = ['_'.join(a) for a in temp1.columns.values]
temp1
```

#### 画图
```python
ax = sns.distplot(train.loc[train.Season==2017,'Orientation'].fillna(0),label = '2017')
ax = sns.distplot(train.loc[train.Season==2018,'Orientation'].fillna(0),label = '2018')
ax = sns.distplot(test.loc[test.Season==2019,'Orientation'].fillna(0),label = '2019')
plt.legend(prop={'size': 12})
```

#### lgb调参
```python
lgb.LGBMClassifier(n_estimators=10000, random_state=seed2,subsample=subsample1,
                         colsample_bytree=colsample_bytree1,learning_rate=0.01,importance_type = 'gain',
                 max_depth = -1, num_leaves = int(num_leaves1),min_child_samples=int(min_child_samples1),
                                 min_child_weight = min_child_weight1,min_split_gain = min_split_gain1,
                   bagging_freq=1,reg_alpha = reg_alpha1,reg_lambda = reg_lambda1,n_jobs = -1,metric='None',
                                scale_pos_weight = scale_pos_weight1)
### num_leaves default:31
[32,64,128,256]
###colsample_bytree、subsample
[0.5,0.6,0.7,0.8,0.9,1]
### min_child_weight 1e-3
[0.0001,0.01,0.1]
### min_child_samples 20
[5,10,40,80]
### min_split_gain 0
[0,0.01,0.1,1]
### reg_alpha 0
### reg_lambda 0
### scale_pos_weight

```
#### 贝叶斯调参模板
```python
def lgb_eval(subsample1,colsample_bytree1,num_leaves1,min_child_weight1,min_split_gain1,reg_alpha1,reg_lambda1,min_child_samples1):
    global test_index
    global y_true_array1
    ts = TimeSeriesSplit(n_splits=8)
    impor1 = 0
    resu1_logloss = 0
    resu2_cprs = 0
    y_train_trans = y_to_cate(y_train,bins)
    y_true_array1 = y_true_array(y_train)
    oof_predict = np.zeros((y_train.shape[0],18))
    for i,(train_index, test_index) in enumerate(ts.split(X_train, y_train_trans)):
        if i not in [6,7]:
            continue
        #print(i,len(train_index),len(test_index))
        X_train2= X_train.iloc[train_index,:]
        y_train2= y_train_trans.iloc[train_index]
        X_test2= X_train.iloc[test_index,:]
        y_test2= y_train_trans.iloc[test_index]
        clf = lgb.LGBMClassifier(n_estimators=10000, random_state=47,learning_rate=0.01,importance_type = 'gain',
                         n_jobs = -1,num_leaves=int(num_leaves1),bagging_freq=1,colsample_bytree=colsample_bytree1
                                 ,subsample=subsample1,min_child_weight = min_child_weight1,
                                 min_child_samples = int(min_child_samples1),reg_alpha = reg_alpha1,reg_lambda = reg_lambda1,metric = 'None',\
                                 min_split_gain = min_split_gain1)
        clf.fit(X_train2,y_train2,eval_set = [(X_test2,y_test2)],early_stopping_rounds=100,verbose=0,eval_metric = CRPS_eval)
        #models.append(clf)
        temp_predict_prob = clf.predict_proba(X_test2)
        oof_predict[test_index,:] = temp_predict_prob.copy()
        crps = cal_CPRS(y_true_array1[test_index,:],raw_y_to_array2(temp_predict_prob,num_counts,bins,train_single['dist_to_end'].iloc[test_index]))
        print(crps)
        resu1_logloss += log_loss(y_test2,temp_predict_prob)/2
        resu2_cprs += crps/2
        impor1 += clf.feature_importances_/2
        gc.collect()
    return -resu2_cprs###看最大最小加正负
lgb_opt = BayesianOptimization(lgb_eval, {
                                          'num_leaves1': (15,120),
                                          'subsample1': (0, 1),
                                          'colsample_bytree1': (0,1),
                                            'min_child_weight1':(0,2),
                                            'min_split_gain1':(0,0.2),
                                            'min_child_samples1':(20,300),
                                            'reg_alpha1':(0,3),
                                            'min_split_gain1':(0,1),
                                        'reg_lambda1':(0,3)
                                        })
lgb_opt.maximize(n_iter=25, init_points=5)
lgb_opt.max['target']
lgb_opt.max['params']
```
#### TS-CV分类模板
```python
ts = TimeSeriesSplit(n_splits=10)
impor1 = 0
resu1_logloss = 0
resu2_cprs = 0
y_train_trans = y_to_cate(y_train,bins)
y_true_array1 = y_true_array(y_train)
oof_predict = np.zeros((y_train.shape[0],18))
for i,(train_index, test_index) in enumerate(ts.split(X_train, y_train_trans)):
    if i not in [7,8,9]:
        continue
    #print(i,len(train_index),len(test_index))
    X_train2= X_train.iloc[train_index,:]
    y_train2= y_train_trans.iloc[train_index]
    X_test2= X_train.iloc[test_index,:]
    y_test2= y_train_trans.iloc[test_index]
    clf = lgb.LGBMClassifier(n_estimators=10000, random_state=47,learning_rate=0.01,importance_type = 'gain',
                     n_jobs = -1,num_leaves=20, =1,colsample_bytree=0.5,subsample=1,min_child_weight = 0.1,
                             min_child_samples = 250,reg_alpha = 1.5,reg_lambda = 1)
    clf.fit(X_train2,y_train2,eval_set = [(X_train2,y_train2),(X_test2,y_test2)],early_stopping_rounds=100,verbose=50)
    #models.append(clf)
    temp_predict_prob = clf.predict_proba(X_test2)
    oof_predict[test_index,:] = temp_predict_prob
    crps = cal_CPRS(y_true_array1[test_index,:],raw_y_to_array2(temp_predict_prob,num_counts,bins,train_single['dist_to_end'].iloc[test_index]))
    print(crps)
    resu1_logloss += log_loss(y_test2,temp_predict_prob)/3
    resu2_cprs += crps/3
    impor1 += clf.feature_importances_/3
    gc.collect()
print('mean cprs:',resu2_cprs)
print('oof cprs:',cal_CPRS(y_true_array1,raw_y_to_array2(oof_predict,num_counts,bins,train_single['dist_to_end'])))
print('mean logloss',resu1_logloss)
print('oof logloss',log_loss(y_train_trans,oof_predict))
```


#### interaction features
```python
temp = ['DeviceInfo__P_emaildomain', 
        'card1__card5', 
        'card2__id_20',
        'card5__P_emaildomain', 
        'addr1__card1',
        'addr1__addr2',
        'card1__card2',
        'card2__addr1',
        'card1__P_emaildomain',
        'card2__P_emaildomain',
        'addr1__P_emaildomain',
        'DeviceInfo__id_31',
        'DeviceInfo__id_20',
        'DeviceType__id_31',
        'DeviceType__id_20',
        'DeviceType__P_emaildomain',
        'card1__M4',
        'card2__M4',
        'addr1__M4',
        'P_emaildomain__M4']
for feature in temp:
    f1, f2 = feature.split('__')
    X_train[feature] = X_train[f1].astype(str) + '_' + X_train[f2].astype(str)
    X_test[feature] = X_test[f1].astype(str) + '_' + X_test[f2].astype(str)

    le = LabelEncoder()
    le.fit(list(X_train[feature].astype(str).values) + list(X_test[feature].astype(str).values))
    X_train[feature] = le.transform(list(X_train[feature].astype(str).values))
    X_test[feature] = le.transform(list(X_test[feature].astype(str).values))
```
#### 判断df是否一样
```python
print(hashlib.sha256(X_train.values.tobytes()).hexdigest())
```
#### Adversarial test
```python
features = X_train.columns.tolist()
X_train['target'] = 1
X_test['target'] = 0
train_test = pd.concat([X_train, X_test], axis =0,ignore_index=True)
train1, test1 = train_test_split(train_test, test_size=0.33, random_state=42, shuffle=True)
train_y = train1['target'].values
test_y = test1['target'].values
del train1['target'], test1['target']
gc.collect()
adversarial_result = pd.DataFrame(index=train1.columns,columns=['roc'])
for i in tqdm(features):
    clf=lgb.LGBMClassifier(random_state=47,max_depth=2,metric='auc',n_estimators=1000,importance_type = 'gain')
    clf.fit(np.array(train1[i]).reshape(-1,1),train_y,eval_set = [(np.array(test1[i]).reshape(-1,1),test_y)],early_stopping_rounds=200,verbose=0)
    temp_pred = clf.predict_proba(np.array(test1[i]).reshape(-1,1))[:,1]
    roc1 = roc_auc_score(test_y,temp_pred)
    adversarial_result.loc[i,'roc'] = roc1
adversarial_result.sort_values('roc',ascending=False)
```
```python
#### testing
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import tqdm
import lightgbm as lgb
from sklearn.metrics import roc_auc_score,mean_squared_error,mean_absolute_error,log_loss,confusion_matrix

class Adversarial_test:
    def __init__(self,X_train,X_test,sample_p = None):
        self.X_train = X_train.copy()
        self.X_test = X_test.copy()
        self.X_train['target'] = 1
        self.X_test['target'] = 0
        self.sample_p = sample_p
        #self.features_skip = features_skip
        train_x,test_x,train_y,test_y = self.split_train_test()
        self.train_x = train_x
        self.test_x = test_x
        self.train_y = train_y
        self.test_y = test_y
        self.features = train_x.columns.tolist()
        if sample_p:
            self.sample(sample_p)
    def split_train_test(self):
        train_test = pd.concat([self.X_train, self.X_test], axis =0,ignore_index=True)
        train1, test1 = train_test_split(train_test, test_size=0.33, random_state=42, shuffle=True)
        train_y = train1['target'].values
        test_y = test1['target'].values
        del train1['target'], test1['target']
        return train1, test1, train_y, test_y
    def sample(self,p):
        temp = [i for i in range(self.train_x.shape[0]) if np.random.rand(1)[0]<p]
        self.train_x = self.train_x.iloc[temp]
        self.train_y = self.train_y[temp]
    def test_one_by_one(self,lr = 0.1,verbose = False,verbose_lgb = 100,features_skip = None):
        adversarial_result = pd.DataFrame(index=self.features,columns=['roc'])
        if features_skip:
            self.features = list(set(self.features).difference(set(features_skip)))
        for i in tqdm.tqdm(self.features):
            if verbose:
                print(i)
            clf=lgb.LGBMClassifier(random_state=47,max_depth=2,metric='auc',n_estimators=1000,importance_type = 'gain',learning_rate = lr)
            clf.fit(np.array(self.train_x[i]).reshape(-1,1),self.train_y,eval_set = [(np.array(self.test_x[i]).reshape(-1,1),self.test_y)],early_stopping_rounds=100,verbose=verbose_lgb)
            temp_pred = clf.predict_proba(np.array(self.test_x[i]).reshape(-1,1))[:,1]
            roc1 = roc_auc_score(self.test_y,temp_pred)
            adversarial_result.loc[i,'roc'] = roc1
        adversarial_result = adversarial_result.sort_values('roc',ascending=False)
        self.result_df = adversarial_result
        return adversarial_result
    def test_together(self,lr = 0.1,verbose_lgb = 100,features_skip = None):
        if features_skip:
            self.features = list(set(self.features).difference(set(features_skip)))
        clf=lgb.LGBMClassifier(random_state=47,max_depth=2,metric='auc',n_estimators=1000,importance_type = 'gain',learning_rate = lr)
        clf.fit(self.train_x[self.features],self.train_y,eval_set = [(self.test_x[self.features],self.test_y)],early_stopping_rounds=100,verbose=verbose_lgb)
        adversarial_result = pd.DataFrame(clf.feature_importances_,index=self.features,columns=['roc']).sort_values('roc',ascending=False)
        self.result_df = adversarial_result
        return adversarial_result
```
### 特征选择
#### RFE
```python
X_train1 = X_train.copy()
for num in [10,20,30,40,50,60]:
    X_train = X_train1.copy()
    X_train.drop(list(fea_impor.index[-num:]),axis=1,inplace=True)
    print(num,'beginssssssss-------------------------!')
    impor1 = 0
    resu1_logloss = 0
    resu2_cprs = 0
    y_train_trans = y_to_cate(y_train,bins)
    y_true_array1 = y_true_array(y_train)
    oof_predict = np.zeros((y_train.shape[0],18))
    X_train2= X_train.iloc[train_index,:]
    y_train2= y_train_trans.iloc[train_index]
    X_test2= X_train.iloc[test_index,:]
    y_test2= y_train_trans.iloc[test_index]
    for _ in [0,1,2]:
        clf = lgb.LGBMClassifier(n_estimators=10000, random_state=47,learning_rate=0.01,importance_type = 'gain',
                         n_jobs = -1,num_leaves=20,bagging_freq=1,colsample_bytree=0.5,subsample=1,min_child_weight = 0.1,
                                 min_child_samples = 250,reg_alpha = 1.5,reg_lambda = 1,metric = 'None')
        clf.fit(X_train2,y_train2,eval_set = [(X_test2,y_test2)],early_stopping_rounds=200,verbose=0,eval_metric = CRPS_eval)
        temp_predict_prob = clf.predict_proba(X_test2)
        oof_predict[test_index,:] = temp_predict_prob
        crps = cal_CPRS(y_true_array1[test_index,:],raw_y_to_array2(temp_predict_prob,num_counts,bins,train_single['dist_to_end'].iloc[test_index]))
        print(crps)
        resu1_logloss += log_loss(y_test2,temp_predict_prob)/3
        resu2_cprs += crps/3
        impor1 += clf.feature_importances_/3
        gc.collect()
    print('mean cprs:',resu2_cprs)
    #print('mean logloss',resu1_logloss)
```

#### permutation test

```python 
### https://www.kaggle.com/dansbecker/permutation-importance
np.sum(np.array([0.3,0.35,0.35])*np.array([0.013928773404035562,0.0143338888203267,0.013468660929614691]))
result = pd.DataFrame(index =fea_impor.index,columns=['score_1','score_2','score_3','score_4','score_5','score_6','score_change_mean'] )
temp222=[0.013928773404035562,0.0143338888203267,0.013468660929614691]
weight = [0.3,0.35,0.35]
for i, column in enumerate(fea_impor.index):
    print(i,column,'begin!!!!!!!!!!!!!!!!!!!!!!!!!!')
    score_new = 0
    ts = TimeSeriesSplit(n_splits=8)
    k = 1
    for _ in range(2):
        for ii,(train_index, test_index) in enumerate(ts.split(X_train, y_train_trans)):
            if ii not in [5,6,7]:
                continue
            X_test2= X_train.iloc[test_index,:]
            y_test2= y_train_trans.iloc[test_index]
            X_test111 = X_test2.copy()
            value = X_test111[column].copy()
            X_test111[column] = np.random.permutation(value)
            temp_predict_prob = models[k%3-1].predict_proba(X_test111)
            crps = cal_CPRS(y_true_array1[test_index,:],raw_y_to_array2(temp_predict_prob,num_counts,bins,train_single['dist_to_end'].iloc[test_index]))
            score_new +=crps/2*weight[k%3-1]
            result.loc[column,'score_'+str(k)] = crps - temp222[k%3-1]
            k +=1
    print(score_new-0.013909524433690154)
    result.loc[column,'score_change_mean'] = score_new-0.013909524433690154
    print(column,'ends!!!!!!!!!!!!!!!!!!!!!!!!!!')
result.to_csv('permutation_test_change_dir_11_25.csv',index=True,index_label='feature')
```
#### add del one by one
```python
def feature_add_one_by_one(X_train,y_train,score_func,feature_names,only_add_1 = True):
    features_keep = ['title','world']####一定要的特征
    waitlist = feature_names.copy()
    score = 1.114850042977697###要改 初始
    score_boost = []
    k = 0
    print('-------------------------1th add')
    for feature in feature_names:
        k+=1
        features_keep.append(feature)
        temp_score = get_score(X_train[features_keep],y_train)
        if temp_score < score:            
            print('Step:',k,feature,'works',round(score,7),round(score - temp_score,7))
            score_boost.append(score - temp_score)
            score = temp_score
            waitlist.remove(feature)
            
        else:
            features_keep.remove(feature)
            print('Step:',k,feature,'fails')
    print(features_keep)
    print(score_boost)
    if only_add_1:
        return features_keep
    print('-------------------------1th del')
    k = 0
    temp1 = np.random.permutation(features_keep.copy())
    for feature in temp1:
        if feature in ['title','world']:
            continue
        k+=1
        features_keep.remove(feature)
        temp_score = get_score(X_train[features_keep],y_train)
        if temp_score < score:
            score = temp_score
            print('Step del:',k,feature,'works',score)
            waitlist.append(feature)
        else:
            features_keep.append(feature)
            print('Step del:',k,feature,'fails')
            
    print('------------------------2th add')
    temp2 = np.random.permutation(waitlist.copy())
    k=0
    for feature in temp2:
        k+=1
        features_keep.append(feature)
        temp_score = get_score(X_train[features_keep],y_train)
        if temp_score < score:
            score = temp_score
            print('Step add2:',k,feature,'works',score)
            waitlist.remove(feature)
        else:
            features_keep.remove(feature)
            print('Step add2:',k,feature,'fails')
            
    print('-------------------------2th del')
    k = 0
    temp3 = np.random.permutation(features_keep.copy())
    for feature in temp3:
        if feature in ['title','world']:
            continue
        k+=1
        features_keep.remove(feature)
        temp_score = get_score(X_train[features_keep],y_train)
        if temp_score < score:
            score = temp_score
            print('Step del2:',k,feature,'works',score)
            waitlist.append(feature)
        else:
            features_keep.append(feature)
            print('Step del2:',k,feature,'fails')
    return features_keep,score_boost
```


### 模型保存
https://blog.csdn.net/u012666689/article/details/103577518

### 树模型训练调整参数
```python
#### https://github.com/microsoft/LightGBM/issues/129
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10,
                init_model=gbm,
                valid_sets=lgb_eval,
                callbacks=[lgb.reset_parameter(bagging_fraction=[0.7]*5+[0.6]*5)])

#### https://www.kaggle.com/mlisovyi/lightgbm-hyperparameter-optimisation-lb-0-761
### 2.3.1版本好像行？
def learning_rate_010_decay_power_0995(current_iter): 
    base_learning_rate = 0.1 
    lr = base_learning_rate * np.power(.995, current_iter) 
    return lr if lr > 1e-3 else 1e-3
lgb_re.fit(X_train,y_train,eval_set = [(X_valid,y_valid)],verbose=50,
           eval_metric =lambda y_true, y_pred: [evaluator.feval(y_true, y_pred), rmse(y_true, y_pred)],
           early_stopping_rounds=100,callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])#,categorical_feature=cat_col)
```

### 改变objective add weight
```python
### https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d
def custom_obj(y_true, y_pred):
    residual = (y_true - y_pred).astype("float")
    grad = -2*residual/scale_X
    hess = 2/scale_X
    return grad, hess
lgb_re = lgb.LGBMRegressor(n_estimators=1000, random_state=51,subsample=0.8,
                         colsample_bytree=0.8,learning_rate=0.1,importance_type = 'gain',
                 max_depth = -1, num_leaves = 64,metric='None',bagging_freq=1,n_jobs=-1,
                         first_metric_only = True,objective=custom_obj, )
lgb_re.fit(X_train,y_train,eval_set = [(X_valid,y_valid)],verbose=50,sample_weight = weight_X,
           eval_metric =lambda y_true, y_pred: [evaluator.feval(y_true, y_pred), rmse(y_true, y_pred)],
           early_stopping_rounds=100)#,categorical_feature=cat_col)
```