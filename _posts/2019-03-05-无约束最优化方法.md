---
layout:     post
title:      无约束最优化方法集合
subtitle:   最优化方法
date:       2019-03-05
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 数学
---
### 梯度下降（Gradient descent）

- Target: $\min f(x)$
- $x^{k+1} \leftarrow x^k - \lambda_k\nabla f(x^k)$

#### 算法调优
- 初始值的选择。 对于非凸函数， 梯度下降可能得到局部最优解，需要多尝试几个初始值，选择得到最小值的初始函数。对于凸函数，局部最小值即为全局最小值，则没有这个问题
- 歩长的选择。步长过长，在接近局部最小值点时，可能无法收敛，在其附近来回震荡。步长过短，会导致迭代次数增加，迭代过慢。 通常解决方法，多考虑几个步长，取使其下降最快的步长。或者若在一次迭代中，目标函数上升了，则取原来一半的步长，退回这次迭代前，重新迭代，直到目标函数下降。
- 归一化。对于数据规格及不一样的梯度下降，需要进行归一化处理。否则会产生之字形下降。可以想象一下一个极端椭圆的下降。

#### 梯度下降大家庭
- 批量梯度下降
- 小批量梯度下降
- 随机梯度下降

[参考这里！！！](https://www.cnblogs.com/lliuye/p/9451903.html)

### 牛顿法（Newton-Raphson Method）

A fast approach for solving $f(x)=0$

- starting with an initial estimate $x_0$
- For t = $0,1,\dots$, compute $x_{t+1} = x_t + h_t$, where $h_t = -\frac{f(x_t)}{f'(x_t)}$
- Continue until convergence.

#### why?

Taylor expansion:

$$f(x) = f(x_0) + f'(x_0)(x-x_0)+\frac{(x-x_0)^2}{2}f''(x_1)$$

where $x_1$ lies between $x$ and $x_0$.

If we want to get $f(x)=0$,

$$0=f(x) \approx f(x_0) + f'(x_0)(x-x_0)\text{ as }x \approx x_0\\
x \approx x_0 -\frac{f(x_0)}{f'(x_0)}$$

#### Application

Usually, we use this to get $f'(x) =0 $, since we want to $\min \text{ or }\max f(x)$.

Then, it comes:

$$x^{t+1} = x^t - \frac{f'(x^t)}{f''(x^t)}$$

**Multivariate Case**

$$x_{t+1}=x_t - [g''(x_t)]^{-1}g'(x_t)$$

where
- $g''(x)$ is a p X p matrix with(i,j) element as $\frac{\partial^2g(x)}{\partial x_i\partial x_j}$. 

- $g'(x) = [\frac{\partial g(x)}{\partial x_1},\dots,\frac{\partial g(x)}{\partial x_p}]$. 

#### 优缺点

- 优点：收敛速度快，Hessian矩阵的逆在迭代过程中不断减少，相当于起到减少步长的效果
- 缺点：计算hessian矩阵的逆复杂度较高。当Hessian矩阵接近奇异时，无法计算

### 拟牛顿法

计算Hessian矩阵或者其逆有时候会很复杂，这时候就产生了拟牛顿法。

将Hessian矩阵用一些简单的可计算的矩阵$M(x)$头代替，然后

$$x_{t+1}=x_t - M_t^{-1}g'(x_t)$$

这里拟牛顿法就介绍一个最简单的例子.

#### Steepest descent Method

- Set $M_t=\alpha_t^{-1}I_p$, where $I_p$ is the $p \times p$ identity matrix and $\alpha_t > 0$ is the step size which can shrink to ensure ascent. Then:

$$x_{t+1} = x_t - \alpha_tg'(x_t)$$

- If at step t, the original step turns out to be uphill, i.e. if $g(x_{t+1}) > g(x_t)$, the updating can be backtracked by halfing $\alpha_t$.
