---
layout:     post
title:      感知机模型
subtitle:   感知机模型
date:       2018-09-30
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 统计机器学习
---

# 前言
第二章介绍了感知机模型，是一种二分类的线性分类模型。

# 第2章 感知机
## 2.1感知机模型

-  定义：假设输入空间（特征空间）是$X \subseteq R^n$,输出空间是\{+1,-1}, x 表示实例的特征向量，对应于输入空间的点：

$$f(x)= sign (w·x+b),w\in R^n,b\in R$$

## 2.2感知机学习策略

- 定义：数据集的线性可分性（数据能用线性分割完全分开）
- 如何去找分割线?定义损失函数并将其最小化得到。
- 定义损失函数：一个自然的选择是误分类点总数，但其不是参数的连续可导函数，不易优化，所以可采用误分类点到超平面的总距离定义损失函数。
- 定义 输入空间$R^n$中任一点$x_0$超平面S的距离：$\frac{1}{\mid \mid w \mid \mid}\mid w·x_0+b \mid$
- 其次，对于误分类的数据$(x_i,y_i)$来说，$-y_i(wx_i+b)>0$,所以误分类点$x_i$到超平面S的距离是$-\frac{1}{\mid \mid w\mid \mid}y_i(wx_i+b)$.假设超平面S的误分类点集合为M。所有误分类点到超平面S的总距离为$-\frac{1}{\mid \mid w\mid \mid}\sum_{x_i \in M}y_i(wx_i+b)$是w,b的连续可导函数。可用梯度下降法求解。
- **再次回顾感知机的时候，突然发现一个问题$L(w,b)$中舍去的分母的原因并不是控制其等于1，之前一直以为是这个原因。。其原因应该在于感知机作用于线性可分空间，所以其L一定能最小化至0，所以加不加分母其一定能收敛到0。换句话说，感知机是误分类驱动的，因此只要损失函数随着误分类点的个数减少，且能减小到0，这就是一个可行的损失函数，因此直接省掉分母是可行的。**[参考知乎](https://www.zhihu.com/question/36241719)
## 2.3感知机学习算法

- 感知机学习中采用随机梯度下降法(stochastic gradient descent)。梯度下降及随机梯度下降[reference1](https://www.cnblogs.com/sirius-swu/p/6932583.html) [reference2](https://www.cnblogs.com/pinard/p/5970503.html).

### 2.3.1 感知机学习算法

- 给定一个训练集$T={(x_1,y_1)},\dots,(x_N,y_N),y_i \in y=\{-1,+1\}$求参数w,b，使其为以下损失函数极小化问题解法：
$$\min_{w,b}L(w,b)=-\sum_{x_i\in M}y_i(wx_i+b)$$

感知机学习算法是误分类驱动的，具体采用随机梯度下降法。首先，任意选取一个超平面$w_0,b_0$,然后用梯度下降法不断地极小化目标函数。极小化过程是不一次使M中所有误分类点的梯度下降，而是随机选取一个误分类点的梯度下降。

$$\nabla_wL(w,b)=-\sum_{x_i \in M}y_ix_i,\nabla_bL(w,b)=-\sum_{x_i \in M}y_i$$

随机选取一个误分类点$(x_i,y_i)$,对w,b进行更新。
$$w \leftarrow w+\eta y_i x_i,  b \leftarrow b+\eta y_i$$

式子中$\eta$是步长，又称学习率(learning rate)

算法（感知机学习算法的原始形式）：

输入：训练集T，学习率$\eta$
输出：w,b：感知机模型f(x)=sign(wx+b)
1. 拟定初值$w_0,b_0$
2. 在T中选取数据$(x_i,y_i)$
3. 如果$y_i(wx_i+b)\leq 0$, $w \leftarrow w+\eta y_i x_i,  b \leftarrow b+\eta y_i$
4. 回到步骤2，直至没有误分类点

直观解释：当一个实例点被误分类，即位于分离超平面的错误的一侧时，则调整参数使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面之间的距离。

### 2.3.2算法的收敛性

可证明，对于线性可数据集感知机学习算法原始形式收敛，即经过有限次迭代可有得到一个将训练数据集完全正确划分开的分离超平面及感知机模型。

### 2.3.3 感知机学习算法的对偶形式

在原始形式的算法中可假设参数初始值都为0，对误分类点$(x_i,y_i)$通过$w \leftarrow w+\eta y_i x_i,  b \leftarrow b+\eta y_i$逐步修改w,b，设修改了n次，则w,b关于$(x_i,y_i)$的增量分别是$\alpha_iy_ix_i$和$\alpha_iy_i$.这里$\alpha_i=n_i\eta$.最后学习到的w,b可以分别表示为$w=\sum_{i=1}^N\alpha_iy_ix_i,b=\sum_{i=1}^N\alpha_iy_i$.

当n=1时，$\alpha_i$表示第i个实例点由于误分类而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近,也就越难正确分类。

算法(感知机学习算法的对偶形式)
输入：线性可分的数据集T；学习率$\eta$
输出：$\alpha$,b.模型$f(x)=sign(\sum_{j=1}^N \alpha_jy_jx_j·x+b)$
1. $\alpha \leftarrow 0 , b\leftarrow 0$
2. 在训练集中选取数据$(x_i,y_i)$
3. 如果$y_i(\sum_{j=1}^N \alpha_jy_jx_jx_i+b)\leq$0,那么$\alpha_i \leftarrow \alpha_i+\eta,b \leftarrow b+\eta y_i$
4. 回到步骤2 直到没有误分类为止

对偶形式中训练实例仅以内积形式出现，计算时可以先计算Gram矩阵。$G=[x_i·x_j]_{N\times N}$

python实现：
```python
# coding: utf-8

import numpy as np

##定义训练集
x=np.array([[3,3],[4,3],[1,1]])
y=np.array([1,1,-1])
##定义初始参数
w=np.array([0,0])
b=0

##原始形式
def per(x,y,w,b,n):
    set_wrong=[1]
    while len(set_wrong)!=0:
        set_wrong=[]
        for i in range(len(x)):
            if y[i]*(w @ x[i]+b)<=0:
                set_wrong.append(i)
        if len(set_wrong)==0:
            return(w,b)
        else:
            temp=set_wrong[np.random.randint(len(set_wrong))]
            w=w+n*y[temp]*x[temp]
            b=b+n*y[temp]
per(x,y,w,b,1)

##对偶形式

a=np.array([[0,0,0],[0,0,0],[0,0,0]])
b=0

def per2(x,y,a,b,n):
    set_wrong=[1]
    while len(set_wrong)!=0:
        set_wrong=[]
        for i in range(len(x)):
            if y[i]*((x.T @a @y)@x[i]+b)<=0:
                set_wrong.append(i)
        if len(set_wrong)==0:
            return(x.T @a @y,b)
        else:
            temp=set_wrong[np.random.randint(len(set_wrong))]
            a[temp,temp]=a[temp,temp]+n
            b=b+n*y[temp]

per2(x,y,a,b,1)

```
### 参考

- 统计学习方法 第二章