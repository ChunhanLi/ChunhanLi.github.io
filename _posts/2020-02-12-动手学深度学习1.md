---
layout:     post
title:      动手学深度学习1
subtitle:   
date:       2020-02-12
author:     Midone
header-img: img/post-bg-re-vs-ng2.jpg
catalog: True
tags:
    - 动手学深度学习打卡
---

### 线性回归

#### 梯度下降

- Target: $\min f(x)$
- $x^{k+1} \leftarrow x^k - \lambda_k\nabla f(x^k)$

#### 算法调优
- 初始值的选择。 对于非凸函数， 梯度下降可能得到局部最优解，需要多尝试几个初始值，选择得到最小值的初始函数。对于凸函数，局部最小值即为全局最小值，则没有这个问题
- 歩长的选择。步长过长，在接近局部最小值点时，可能无法收敛，在其附近来回震荡。步长过短，会导致迭代次数增加，迭代过慢。 通常解决方法，多考虑几个步长，取使其下降最快的步长。或者若在一次迭代中，目标函数上升了，则取原来一半的步长，退回这次迭代前，重新迭代，直到目标函数下降。
- 归一化。对于数据规格及不一样的梯度下降，需要进行归一化处理。否则会产生之字形下降。可以想象一下一个极端椭圆的下降。

#### 梯度下降大家庭
- 批量梯度下降
- 小批量梯度下降
- 随机梯度下降

[参考这里！！！](https://www.cnblogs.com/lliuye/p/9451903.html)

### Softmax与分类模型

若$o_1,o_2,o_3$分别为输出1,2,3，那么

$$\hat y_1 = \frac{\exp(o_1)}{\sum_{i=1}^3\exp(o_i)},\hat y_2 = \frac{\exp(o_2)}{\sum_{i=1}^3\exp(o_i)},\hat y_3 = \frac{\exp(o_3)}{\sum_{i=1}^3\exp(o_i)}$$

- $\hat y_1,\hat y_2,\hat y_3$之和等于1，满足概率定义
- $\arg_i \max o_i = \arg_i \max y_i$

### 多层感知机

- 由于多个仿射变换还是仿射变换。我们需要引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。
- ReLu函数/Sigmoid函数/tanh函数

#### 关于激活函数的选择
- ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。
- 用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。
- 在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。
- 在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。